Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 15: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 54: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 95: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xf0 in position 5: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe8 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 17: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 14: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 27: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 1: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 323: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 20: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 128: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 31: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 2: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 2: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 15: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 285: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0x97 in position 18: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0x97 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 1: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 243: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xd7 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 3: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 15: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 2: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 16: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 8: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 14: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 15: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 1: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 8: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 27: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 17: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 8: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 23: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 27: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 163, in goahead
    k = self.parse_endtag(i)
  File "C:\Python27\lib\HTMLParser.py", line 401, in parse_endtag
    self.handle_endtag(elem)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 34, in handle_endtag
    if tag == self.tag_stack[-1]:
IndexError: list index out of range



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 8: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 1: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 8: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 14: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 15: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 54: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 11: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 178: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe8 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 17: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 34: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0x80 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 32: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 38: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 37: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 165: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0x80 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 32: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 1: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 243: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 3: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 14: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 323: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 20: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 27: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 128: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 2: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xd7 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 27: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 18: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 15: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 54: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 43: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 17: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 2: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 2: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 8: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 95: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 40: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 14: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 39: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 16: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 89: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 14: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 243: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 3: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 67: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)



Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 323: ordinal not in range(128)



