visiting http://www.vice.com/en_us
	priority is 0
	the current length of the visited set is : 0
crawled http://www.vice.com/en_us it wasn't  news 
returning reqs
visiting http://www.vice.com/read/artists-of-today-part-2
	priority is -3
	the current length of the visited set is : 1
crawled http://www.vice.com/read/artists-of-today-part-2 it wasn't  news 
returning reqs
visiting http://www.vice.com/articles
	priority is -3
	the current length of the visited set is : 2
crawled http://www.vice.com/articles it wasn't  news 
returning reqs
visiting http://noisey.vice.com/blog/getting-rid-of-your-vinyl-forever-heres-a-cookie
	priority is 0
	the current length of the visited set is : 3
crawled http://noisey.vice.com/blog/getting-rid-of-your-vinyl-forever-heres-a-cookie it wasn't  news 
returning reqs
visiting http://www.vice.com/comics
	priority is -3
	the current length of the visited set is : 4
crawled http://www.vice.com/comics it wasn't  news 
returning reqs
visiting https://news.vice.com/article/islamic-state-warns-of-black-days-for-saudi-arabia-as-bomber-in-mosque-attack-is-identified
	priority is 0
	the current length of the visited set is : 5
crawled https://news.vice.com/article/islamic-state-warns-of-black-days-for-saudi-arabia-as-bomber-in-mosque-attack-is-identified it wasn't  news 
returning reqs
visiting http://www.vice.com/author/batya-ungarsargon
	priority is -3
	the current length of the visited set is : 6
crawled http://www.vice.com/author/batya-ungarsargon it wasn't  news 
returning reqs
visiting http://motherboard.vice.com/read/isis-vs-3d-printing
	priority is 0
	the current length of the visited set is : 7
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 15: ordinal not in range(128)

visiting http://link.vice.com/join/3nh/viceussignup
	priority is 0
	the current length of the visited set is : 8
crawled http://link.vice.com/join/3nh/viceussignup it wasn't  news 
returning reqs
visiting http://munchies.vice.com/articles/why-is-brooklyn-barbecue-taking-over-the-world
	priority is 0
	the current length of the visited set is : 9
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 54: ordinal not in range(128)

visiting http://www.vice.com/read/a-tech-guy-from-detroit-created-a-dating-app-that-matches-israelis-with-palestinians
	priority is -3
	the current length of the visited set is : 10
crawled http://www.vice.com/read/a-tech-guy-from-detroit-created-a-dating-app-that-matches-israelis-with-palestinians it wasn't  news 
returning reqs
visiting http://store.vice.com/
	priority is 0
	the current length of the visited set is : 11
crawled http://store.vice.com/ it wasn't  news 
returning reqs
visiting https://news.vice.com/article/protesters-are-going-to-hit-the-streets-this-weekend-in-over-400-cities-to-march-against-monsanto
	priority is 0
	the current length of the visited set is : 12
crawled https://news.vice.com/article/protesters-are-going-to-hit-the-streets-this-weekend-in-over-400-cities-to-march-against-monsanto it was  news 
returning reqs
visiting http://noisey.vice.com/blog/some-nerds-taught-a-computer-how-to-rap-deepbeat
	priority is 0
	the current length of the visited set is : 13
crawled http://noisey.vice.com/blog/some-nerds-taught-a-computer-how-to-rap-deepbeat it wasn't  news 
returning reqs
visiting http://www.vice.com/read/the-us-military-euthanized-or-abandoned-thousands-of-their-own-canine-soldiers-at-the-end-of-the-vietnam-war-253
	priority is -3
	the current length of the visited set is : 14
crawled http://www.vice.com/read/the-us-military-euthanized-or-abandoned-thousands-of-their-own-canine-soldiers-at-the-end-of-the-vietnam-war-253 it was  news 
returning reqs
visiting http://killeracid.com/
	priority is 0
	the current length of the visited set is : 15
crawled http://killeracid.com/ it wasn't  news 
returning reqs
visiting http://motherboard.vice.com/terms
	priority is 0
	the current length of the visited set is : 16
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 7: ordinal not in range(128)

visiting http://www.asheavenue.com/
	priority is 0
	the current length of the visited set is : 17
crawled http://www.asheavenue.com/ it wasn't  news 
returning reqs
visiting http://www.vice.com/author/aleksandar-hemon
	priority is -3
	the current length of the visited set is : 18
crawled http://www.vice.com/author/aleksandar-hemon it wasn't  news 
returning reqs
visiting http://munchies.vice.com/articles/avocados-might-help-protect-your-lungs-from-stinky-air-pollution
	priority is 0
	the current length of the visited set is : 19
crawled http://munchies.vice.com/articles/avocados-might-help-protect-your-lungs-from-stinky-air-pollution it wasn't  news 
returning reqs
visiting https://twitter.com/vice
	priority is 0
	the current length of the visited set is : 20
crawled https://twitter.com/vice it wasn't  news 
returning reqs
visiting http://noisey.vice.com/privacy-policy
	priority is 0
	the current length of the visited set is : 21
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting http://www.vice.com/read/the-inception-of-zombie-wars-0000654-v22n5
	priority is -3
	the current length of the visited set is : 22
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)

visiting http://noisey.vice.com/terms-of-use
	priority is 0
	the current length of the visited set is : 23
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)

visiting http://noisey.vice.com/tag/The+Matrix
	priority is 0
	the current length of the visited set is : 24
crawled http://noisey.vice.com/tag/The+Matrix it wasn't  news 
returning reqs
visiting https://news.vice.com/article/microbeads-kill-animals-and-destroy-the-environment-so-california-may-ban-them
	priority is 0
	the current length of the visited set is : 25
crawled https://news.vice.com/article/microbeads-kill-animals-and-destroy-the-environment-so-california-may-ban-them it was  news 
returning reqs
visiting http://www.getverona.com/
	priority is 0
	the current length of the visited set is : 26
crawled http://www.getverona.com/ it wasn't  news 
returning reqs
visiting http://www.vice.com/stuff
	priority is -3
	the current length of the visited set is : 27
crawled http://www.vice.com/stuff it wasn't  news 
returning reqs
visiting https://twitter.com/josiahmhesse
	priority is 2
	the current length of the visited set is : 28
crawled https://twitter.com/josiahmhesse it wasn't  news 
returning reqs
visiting http://rt.com/news/175792-jews-arabs-refuse-enemies/
	priority is 0
	the current length of the visited set is : 29
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)

visiting http://noisey.vice.com/tag/Redrama
	priority is 0
	the current length of the visited set is : 30
crawled http://noisey.vice.com/tag/Redrama it wasn't  news 
returning reqs
visiting http://www.rollingstone.com/music/news/neil-young-recording-new-album-with-willie-nelsons-sons-20150108
	priority is 2
	the current length of the visited set is : 31
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 95: ordinal not in range(128)

visiting http://www.vice.com/author/charlie-ambler
	priority is -3
	the current length of the visited set is : 32
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)

visiting http://www.vice.com/read/vice-exclusive-check-out-unsung-80s-post-punk-heroes-the-mothmen-with-their-new-reissue-815
	priority is -3
	the current length of the visited set is : 33
crawled http://www.vice.com/read/vice-exclusive-check-out-unsung-80s-post-punk-heroes-the-mothmen-with-their-new-reissue-815 it wasn't  news 
returning reqs
visiting https://instagram.com/killeracid/
	priority is 0
	the current length of the visited set is : 34
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xf0 in position 5: ordinal not in range(128)

visiting https://news.vice.com/article/video-shows-frenzied-clash-at-protest-over-plan-to-build-massive-copper-mine-in-peru
	priority is 0
	the current length of the visited set is : 35
crawled https://news.vice.com/article/video-shows-frenzied-clash-at-protest-over-plan-to-build-massive-copper-mine-in-peru it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/great-plastic-garbage-patch
	priority is 2
	the current length of the visited set is : 36
crawled https://news.vice.com/topic/great-plastic-garbage-patch it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140519-dogs-war-canines-soldiers-troops-military-vietnam/
	priority is 2
	the current length of the visited set is : 37
crawled http://news.nationalgeographic.com/news/2014/05/140519-dogs-war-canines-soldiers-troops-military-vietnam/ it was  news 
returning reqs
visiting http://www.vice.com/video/vice-meets-anas-aremeyaw-anas
	priority is -3
	the current length of the visited set is : 38
crawled http://www.vice.com/video/vice-meets-anas-aremeyaw-anas it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/Eminem
	priority is 0
	the current length of the visited set is : 39
crawled http://noisey.vice.com/tag/Eminem it wasn't  news 
returning reqs
visiting http://www.cnn.com/2010/LIVING/02/12/war.dogs/
	priority is 2
	the current length of the visited set is : 40
crawled http://www.cnn.com/2010/LIVING/02/12/war.dogs/ it wasn't  news 
returning reqs
visiting http://munchies.vice.com/tag/brooklyn
	priority is 0
	the current length of the visited set is : 41
crawled http://munchies.vice.com/tag/brooklyn it wasn't  news 
returning reqs
visiting http://www.denverpost.com/news/ci_6003386
	priority is 2
	the current length of the visited set is : 42
crawled http://www.denverpost.com/news/ci_6003386 it wasn't  news 
returning reqs
visiting https://www.facebook.com/VICE
	priority is 0
	the current length of the visited set is : 43
crawled https://www.facebook.com/VICE it wasn't  news 
returning reqs
visiting https://plus.google.com/+VICE
	priority is 0
	the current length of the visited set is : 44
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe8 in position 0: ordinal not in range(128)

visiting https://twitter.com/5gyres/statuses/598708102972715008
	priority is 2
	the current length of the visited set is : 45
crawled https://twitter.com/5gyres/statuses/598708102972715008 it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/plastic
	priority is 2
	the current length of the visited set is : 46
crawled https://news.vice.com/topic/plastic it wasn't  news 
returning reqs
visiting http://www.vice.com/film
	priority is -3
	the current length of the visited set is : 47
crawled http://www.vice.com/film it wasn't  news 
returning reqs
visiting https://twitter.com/HammerDaily
	priority is 2
	the current length of the visited set is : 48
crawled https://twitter.com/HammerDaily it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/pollution
	priority is 2
	the current length of the visited set is : 49
crawled https://news.vice.com/topic/pollution it wasn't  news 
returning reqs
visiting http://www.vice.com/read/canadas-truth-commission-on-residential-schools-is-coming-to-a-troubling-close-far-from-reconciliation
	priority is -3
	the current length of the visited set is : 50
crawled http://www.vice.com/read/canadas-truth-commission-on-residential-schools-is-coming-to-a-troubling-close-far-from-reconciliation it was  news 
returning reqs
visiting https://news.vice.com/topic/oceans
	priority is 2
	the current length of the visited set is : 51
crawled https://news.vice.com/topic/oceans it wasn't  news 
returning reqs
visiting https://www.youtube.com/user/vice
	priority is 0
	the current length of the visited set is : 52
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 17: ordinal not in range(128)

visiting https://twitter.com/NatGeo
	priority is 2
	the current length of the visited set is : 53
crawled https://twitter.com/NatGeo it wasn't  news 
returning reqs
visiting https://www.tumblr.com/register/follow/vicemag
	priority is 0
	the current length of the visited set is : 54
crawled https://www.tumblr.com/register/follow/vicemag it wasn't  news 
returning reqs
visiting http://www.vice.com/read/the-sand-looters-0000647-v22n5
	priority is -3
	the current length of the visited set is : 55
crawled http://www.vice.com/read/the-sand-looters-0000647-v22n5 it wasn't  news 
returning reqs
visiting http://www.vice.com/read/pc-andrew-ott-conviction-235
	priority is -3
	the current length of the visited set is : 56
crawled http://www.vice.com/read/pc-andrew-ott-conviction-235 it was  news 
returning reqs
visiting http://www.safetyandcarecommitment.com/ingredient-info/other/microbeads
	priority is 2
	the current length of the visited set is : 57
crawled http://www.safetyandcarecommitment.com/ingredient-info/other/microbeads it wasn't  news 
returning reqs
visiting http://storyofstuff.org/movies/
	priority is 2
	the current length of the visited set is : 58
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://asmdc.org/members/a50/news-room/press-releases/legislation-to-ban-plastic-microbeads-in-care-products-passes-committee
	priority is 2
	the current length of the visited set is : 59
crawled http://asmdc.org/members/a50/news-room/press-releases/legislation-to-ban-plastic-microbeads-in-care-products-passes-committee it wasn't  news 
returning reqs
visiting http://www.leginfo.ca.gov/pub/15-16/bill/asm/ab_0851-0900/ab_888_bill_20150226_introduced.html
	priority is 2
	the current length of the visited set is : 60
crawled http://www.leginfo.ca.gov/pub/15-16/bill/asm/ab_0851-0900/ab_888_bill_20150226_introduced.html it wasn't  news 
returning reqs
visiting https://news.vice.com/article/%3C/p
	priority is 2
	the current length of the visited set is : 61
crawled https://news.vice.com/article/%3C/p it wasn't  news 
returning reqs
visiting http://www.washingtonpost.com/news/to-your-health/wp/2014/09/18/why-dentists-are-speaking-out-about-the-plastic-beads-in-your-toothpaste/
	priority is 2
	the current length of the visited set is : 62
crawled http://www.washingtonpost.com/news/to-your-health/wp/2014/09/18/why-dentists-are-speaking-out-about-the-plastic-beads-in-your-toothpaste/ it wasn't  news 
returning reqs
visiting http://education.nationalgeographic.com/education/encyclopedia/great-pacific-garbage-patch/?ar_a=1
	priority is 0
	the current length of the visited set is : 63
crawled http://education.nationalgeographic.com/education/encyclopedia/great-pacific-garbage-patch/?ar_a=1 it wasn't  news 
returning reqs
visiting https://www.facebook.com/natgeo
	priority is 2
	the current length of the visited set is : 64
crawled https://www.facebook.com/natgeo it wasn't  news 
returning reqs
visiting http://www.beatthemicrobead.org/en/science
	priority is 2
	the current length of the visited set is : 65
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 14: ordinal not in range(128)

visiting https://twitter.com/mcslo
	priority is 2
	the current length of the visited set is : 66
crawled https://twitter.com/mcslo it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/community/privacy/
	priority is 2
	the current length of the visited set is : 67
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://www.marketplace.org/topics/business/pollutants-your-face-wash
	priority is 2
	the current length of the visited set is : 68
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 9: ordinal not in range(128)

visiting http://www.nationalgeographic.com/community/terms/
	priority is 2
	the current length of the visited set is : 69
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting https://news.vice.com/topic/greenpeace
	priority is 2
	the current length of the visited set is : 70
crawled https://news.vice.com/topic/greenpeace it wasn't  news 
returning reqs
visiting http://events.nationalgeographic.com/
	priority is 2
	the current length of the visited set is : 71
crawled http://events.nationalgeographic.com/ it wasn't  news 
returning reqs
visiting https://dev.twitter.com/web/embedded-tweets
	priority is 0
	the current length of the visited set is : 72
crawled https://dev.twitter.com/web/embedded-tweets it wasn't  news 
returning reqs
visiting https://twitter.com/WailQ
	priority is 2
	the current length of the visited set is : 73
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting https://instagram.com/natgeo/
	priority is 0
	the current length of the visited set is : 74
crawled https://instagram.com/natgeo/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/
	priority is 2
	the current length of the visited set is : 75
crawled http://press.nationalgeographic.com/ it was  news 
returning reqs
visiting http://www.vice.com/pages/privacy-and-terms
	priority is -3
	the current length of the visited set is : 76
crawled http://www.vice.com/pages/privacy-and-terms it wasn't  news 
returning reqs
visiting http://shop.nationalgeographic.com/ngs/index.jsp
	priority is 2
	the current length of the visited set is : 77
crawled http://shop.nationalgeographic.com/ngs/index.jsp it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/siteindex/customer/
	priority is 2
	the current length of the visited set is : 78
crawled http://www.nationalgeographic.com/siteindex/customer/ it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/mediakit/
	priority is 2
	the current length of the visited set is : 79
crawled http://www.nationalgeographic.com/mediakit/ it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/community/email/
	priority is 2
	the current length of the visited set is : 80
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting http://events.nationalgeographic.com/exhibits/
	priority is 0
	the current length of the visited set is : 81
crawled http://events.nationalgeographic.com/exhibits/ it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/magazines/
	priority is 2
	the current length of the visited set is : 82
crawled http://www.nationalgeographic.com/magazines/ it wasn't  news 
returning reqs
visiting http://www.vice.com/pages/jobs
	priority is -3
	the current length of the visited set is : 83
crawled http://www.vice.com/pages/jobs it wasn't  news 
returning reqs
visiting http://www.canada.com/victoriatimescolonist/news/comment/story.html
	priority is 2
	the current length of the visited set is : 84
crawled http://www.canada.com/victoriatimescolonist/news/comment/story.html it wasn't  news 
returning reqs
visiting http://www.nationalgeographicexpeditions.com/
	priority is 0
	the current length of the visited set is : 85
crawled http://www.nationalgeographicexpeditions.com/ it wasn't  news 
returning reqs
visiting http://www.vice.com/pages/about
	priority is -3
	the current length of the visited set is : 86
crawled http://www.vice.com/pages/about it wasn't  news 
returning reqs
visiting https://support.twitter.com/forums/26810/entries/78525
	priority is 0
	the current length of the visited set is : 87
crawled https://support.twitter.com/forums/26810/entries/78525 it wasn't  news 
returning reqs
visiting http://www.canada.com/ottawacitizen/story.html
	priority is 2
	the current length of the visited set is : 88
crawled http://www.canada.com/ottawacitizen/story.html it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/canada/prime-minister-stephen-harper-s-statement-of-apology-1.734250
	priority is 2
	the current length of the visited set is : 89
crawled http://www.cbc.ca/news/canada/prime-minister-stephen-harper-s-statement-of-apology-1.734250 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/canada/manitoba/canada-s-truth-commission-learned-from-mandela-says-head-1.2454851
	priority is 2
	the current length of the visited set is : 90
crawled http://www.cbc.ca/news/canada/manitoba/canada-s-truth-commission-learned-from-mandela-says-head-1.2454851 it wasn't  news 
returning reqs
visiting http://www.theglobeandmail.com/news/national/in-photos-st-michaels-residential-school-demolition-ceremony/article23067843/
	priority is 2
	the current length of the visited set is : 91
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)

visiting http://www.cbc.ca/news/canada/manitoba/truth-is-hard-but-residential-school-reconciliation-harder-murray-sinclair-1.2819931
	priority is 2
	the current length of the visited set is : 92
crawled http://www.cbc.ca/news/canada/manitoba/truth-is-hard-but-residential-school-reconciliation-harder-murray-sinclair-1.2819931 it wasn't  news 
returning reqs
visiting http://www.scielo.org.za/scielo.php
	priority is 2
	the current length of the visited set is : 93
crawled http://www.scielo.org.za/scielo.php it wasn't  news 
returning reqs
visiting http://www.thestar.com/news/canada/2012/06/11/four_years_later_harpers_apology_for_residential_schools_rings_hollow_for_many.html
	priority is 2
	the current length of the visited set is : 94
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting http://www.theglobeandmail.com/news/politics/at-reconciliation-commission-key-positions-remain-unfilled/article4313620/
	priority is 2
	the current length of the visited set is : 95
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 27: ordinal not in range(128)

visiting https://donate.nationalgeographic.org/Page.aspx
	priority is 2
	the current length of the visited set is : 96
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 1: ordinal not in range(128)

visiting http://www.thestar.com/news/canada/2010/07/23/no_truth_no_reconciliation_for_aging_residential_school_survivors.html
	priority is 2
	the current length of the visited set is : 97
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)

visiting http://www.defendtherighttoprotest.org/police-on-trial-for-2010-student-fees-protests-ipcc-release-audio-recording/
	priority is 2
	the current length of the visited set is : 98
crawled http://www.defendtherighttoprotest.org/police-on-trial-for-2010-student-fees-protests-ipcc-release-audio-recording/ it wasn't  news 
returning reqs
visiting http://rabble.ca/blogs/bloggers/derrick/2009/09/harper-denial-g20-canada-has-no-history-colonialism
	priority is 2
	the current length of the visited set is : 99
crawled http://rabble.ca/blogs/bloggers/derrick/2009/09/harper-denial-g20-canada-has-no-history-colonialism it wasn't  news 
returning reqs
visiting http://aptn.ca/news/2013/04/30/trc-ready-to-again-take-residential-school-document-fight-with-ottawa-to-court/
	priority is 2
	the current length of the visited set is : 100
crawled http://aptn.ca/news/2013/04/30/trc-ready-to-again-take-residential-school-document-fight-with-ottawa-to-court/ it wasn't  news 
returning reqs
visiting http://www.theguardian.com/environment/true-north/2015/mar/03/documents-harper-pushing-first-nations-to-shelve-rights-buy-into-resource-rush
	priority is 2
	the current length of the visited set is : 101
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://www.cbc.ca/news/politics/ottawa-ordered-to-provide-all-residential-schools-documents-1.1345892
	priority is 2
	the current length of the visited set is : 102
crawled http://www.cbc.ca/news/politics/ottawa-ordered-to-provide-all-residential-schools-documents-1.1345892 it was  news 
returning reqs
visiting http://www.cbc.ca/news/canada/british-columbia/thousands-walk-for-reconciliation-in-vancouver-1.1864051
	priority is 2
	the current length of the visited set is : 103
crawled http://www.cbc.ca/news/canada/british-columbia/thousands-walk-for-reconciliation-in-vancouver-1.1864051 it wasn't  news 
returning reqs
visiting http://reconciliationcanada.ca/welcome/who-we-are/
	priority is 2
	the current length of the visited set is : 104
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 323: ordinal not in range(128)

visiting http://www.vice.com/tag/lgbt
	priority is -3
	the current length of the visited set is : 105
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting https://www.youtube.com/oops
	priority is 0
	the current length of the visited set is : 106
crawled https://www.youtube.com/oops it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/page/2/
	priority is 2
	the current length of the visited set is : 107
crawled http://press.nationalgeographic.com/page/2/ it was  news 
returning reqs
visiting http://www.straight.com/news/428331/residential-school-survivors-share-their-stories-truth-and-reconciliation-event-vancouver
	priority is 2
	the current length of the visited set is : 108
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)

visiting http://www.anglican.ca/relationships/apology/english
	priority is 2
	the current length of the visited set is : 109
crawled http://www.anglican.ca/relationships/apology/english it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/culture
	priority is -3
	the current length of the visited set is : 110
crawled http://www.vice.com/tag/culture it wasn't  news 
returning reqs
visiting http://www.vice.com/rss
	priority is -3
	the current length of the visited set is : 111
crawled http://www.vice.com/rss it wasn't  news 
returning reqs
visiting https://www.flickr.com/photos/aadncanada/16731932098/in/photolist-ruxwtU-rM3c7M-ruyJ6S-qQkB6g-rM3bZH-rM1QYJ-9XpAoS-9XmHX4-9XpA39-9XpzUL-rJPvQG-5gCEqy-pJM6s-pJM6C-ruyHAy-qQkB3R-ruxvYL-qQkAzB-5gGtiT-q32UX-pJM6v-gub4Cu-omDSt7-qmqQ9E-r2121R-qmEWEM-r1UDLC-r1U35s-qZ6vGP-rirKC4-riqJdc-qmrCFN-r1VKpj-r1UEWm-rinQC9-r22K9g-qms4V5-r1SAB3-rijnCc-qmDbG8-r221aR-qmE9Qa-rik1RR-qmsr3f-r1TRbo-ripdZ1-rijgHB-r1Zdze-riqibZ-qmG4Pz
	priority is 2
	the current length of the visited set is : 112
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 20: ordinal not in range(128)

visiting http://www.cbc.ca/news/aboriginal/residential-school-survivors-event-draws-thousands-in-edmonton-1.2588752
	priority is 2
	the current length of the visited set is : 113
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 128: ordinal not in range(128)

visiting http://www.cccb.ca/site/eng/media-room/files/2630-apology-on-residential-schools-by-the-catholic-church
	priority is 2
	the current length of the visited set is : 114
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 9: ordinal not in range(128)

visiting http://www.vice.com/tag/opinion
	priority is -3
	the current length of the visited set is : 115
crawled http://www.vice.com/tag/opinion it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/paul-martin-accuses-residential-schools-of-cultural-genocide-1.1335199
	priority is 2
	the current length of the visited set is : 116
crawled http://www.cbc.ca/news/politics/paul-martin-accuses-residential-schools-of-cultural-genocide-1.1335199 it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/geo-bee/
	priority is 2
	the current length of the visited set is : 117
crawled http://press.nationalgeographic.com/geo-bee/ it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/politics
	priority is -3
	the current length of the visited set is : 118
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting http://www.vice.com/series/the-vice-guide-to-mental-health
	priority is -3
	the current length of the visited set is : 119
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6: ordinal not in range(128)

visiting https://www.flickr.com/photos/davidstanleytravel/9411102177/in/photolist-fkCmNn-3M6Nd-q2bMhJ-ruwA53-5whnRc-5wmGWs-5wmH25-5wmGUf-5wmGPG-5wmGK1-5wmGYN-5whnzn-5wmGGA-5wmGBG-5whnuM-5wmGEw-5wmH4j-5whnhR-fkSw5d-4Qik69-8LbEMU-5fXpnr-4Qe6Sr-5gGsXk-9R4mgd-apKYx5-9e9Rsh-5Vsx7R-rJPvQG-ruwA1q-denoVd-8fwvFX-6idWX4-7ScLXL-7JcSmt-4x2x8T-5nY6b-kBzC6n-aj1CDk-5RfuuD-pWvnE8-sVekii-qDHPHe-sQ4bW3-na4zsJ-s8sXW-jKrn34-nyhjUR-ppbVJH-3cQcgC
	priority is 2
	the current length of the visited set is : 120
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 31: ordinal not in range(128)

visiting http://press.nationalgeographic.com/2015/05/19/national-geographic-kids-magazine-arrives-in-australia-and-new-zealand-in-june/
	priority is 2
	the current length of the visited set is : 121
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)

visiting https://www.aadnc-aandc.gc.ca/eng/1332949137290/1332949312397
	priority is 2
	the current length of the visited set is : 122
crawled https://www.aadnc-aandc.gc.ca/eng/1332949137290/1332949312397 it wasn't  news 
returning reqs
visiting https://www.aadnc-aandc.gc.ca/eng/1332939430258/1332939552554
	priority is 2
	the current length of the visited set is : 123
crawled https://www.aadnc-aandc.gc.ca/eng/1332939430258/1332939552554 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/canada/rcmp-herded-native-kids-to-residential-schools-1.992618
	priority is 2
	the current length of the visited set is : 124
crawled http://www.cbc.ca/news/canada/rcmp-herded-native-kids-to-residential-schools-1.992618 it was  news 
returning reqs
visiting http://www.amazon.ca/Kill-Indian-Save-Man-Residential/dp/0872864340
	priority is 2
	the current length of the visited set is : 125
crawled http://www.amazon.ca/Kill-Indian-Save-Man-Residential/dp/0872864340 it wasn't  news 
returning reqs
visiting http://www.anglican.ca/news/finding-hildas-grave/3003619/
	priority is 2
	the current length of the visited set is : 126
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting http://www.foxflash.com/national-geographic-channel/
	priority is 2
	the current length of the visited set is : 127
crawled http://www.foxflash.com/national-geographic-channel/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/05/14/national-geographic-and-toms-partner-to-promote-awareness-of-big-cats/
	priority is 2
	the current length of the visited set is : 128
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)

visiting http://www.vice.com/magazine
	priority is -3
	the current length of the visited set is : 129
crawled http://www.vice.com/magazine it wasn't  news 
returning reqs
visiting http://thetyee.ca/Opinion/2013/09/18/Break-Residential-Schools-Dark-Legacy-Understand-Why/
	priority is 2
	the current length of the visited set is : 130
crawled http://thetyee.ca/Opinion/2013/09/18/Break-Residential-Schools-Dark-Legacy-Understand-Why/ it was  news 
returning reqs
visiting http://www.vice.com/tag/crime
	priority is -3
	the current length of the visited set is : 131
crawled http://www.vice.com/tag/crime it wasn't  news 
returning reqs
visiting http://www.anglican.ca/relationships/histories/gordons-school-punnichy
	priority is 2
	the current length of the visited set is : 132
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 2: ordinal not in range(128)

visiting http://www.trc.ca/websites/trcinstitution/index.php?p=26
	priority is 0
	the current length of the visited set is : 133
crawled http://www.trc.ca/websites/trcinstitution/index.php?p=26 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2013/10/18/genocide-first-nations-aboriginals-canada-un_n_4123112.html
	priority is 2
	the current length of the visited set is : 134
crawled http://www.huffingtonpost.ca/2013/10/18/genocide-first-nations-aboriginals-canada-un_n_4123112.html it was  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/04/29/maggie-zackowitz-editor-in-chief-national-geographic-travel/
	priority is 2
	the current length of the visited set is : 135
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)

visiting https://www.flickr.com/photos/9700188@N04/4680817951/in/photolist-qmDbG8-r221aR-qmE9Qa-rik1RR-qmsr3f-r1TRbo-ripdZ1-rijgHB-r1Zdze-riqibZ-qmG4Pz-rioRoL-r1TDU5-qmrcZ3-qmFstp-rirx2T-r1Tc8E-riknP2-rinDpY-rg7F8d-rinr5s-qZ6JvV-r23gxn-r1Ua9A-r1ZSTi-rimRMe-r1U8KL-r1TJqj-qmEEiD-r1UaSb-r1Vapy-r1Ufku-qZ8fER-r1TZRE-r1SE1N-r1SC6A-rimNPm-qmrZCs-qmGs7R-qmFTNx-r1UwHh-qmFnLt-riiKBe-bF8dan-bF8det-bsdjBY-bsdjtj-qZS569-88Cq6M-71Xiit
	priority is 2
	the current length of the visited set is : 136
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 2: ordinal not in range(128)

visiting http://opentextbc.ca/geography/chapter/4-4-case-study/
	priority is 2
	the current length of the visited set is : 137
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 15: ordinal not in range(128)

visiting http://s3.documentcloud.org/documents/563338/fontaine-v-canada-2013-onsc-684.txt
	priority is 2
	the current length of the visited set is : 138
crawled http://s3.documentcloud.org/documents/563338/fontaine-v-canada-2013-onsc-684.txt it was  news 
returning reqs
visiting http://press.nationalgeographic.com/feed/
	priority is 0
	the current length of the visited set is : 139
crawled http://press.nationalgeographic.com/feed/ it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/siteindex/international/
	priority is 2
	the current length of the visited set is : 140
crawled http://www.nationalgeographic.com/siteindex/international/ it wasn't  news 
returning reqs
visiting http://www.thecanadianencyclopedia.ca/en/article/residential-schools/
	priority is 2
	the current length of the visited set is : 141
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)

visiting http://www.vice.com/photos
	priority is -3
	the current length of the visited set is : 142
crawled http://www.vice.com/photos it wasn't  news 
returning reqs
visiting http://www.anglican.ca/relationships/histories/st-michaels-alert-bay
	priority is 2
	the current length of the visited set is : 143
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 285: ordinal not in range(128)

visiting http://indigenousfoundations.arts.ubc.ca/home/government-policy/the-residential-school-system.html
	priority is 2
	the current length of the visited set is : 144
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0x97 in position 18: ordinal not in range(128)

visiting http://www.umista.org/masks_story/en/ht/potlatch01.html
	priority is 2
	the current length of the visited set is : 145
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0x97 in position 10: ordinal not in range(128)

visiting https://genographic.nationalgeographic.com/press/
	priority is 2
	the current length of the visited set is : 146
crawled https://genographic.nationalgeographic.com/press/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/05/04/1-billion-likes-17-million-followers-instagram-top-non-celebrity-account/
	priority is 2
	the current length of the visited set is : 147
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://press.nationalgeographic.com/2015/05/13/karan-menon-new-jersey-wins-2015-national-geographic-bee-scholarship/
	priority is 2
	the current length of the visited set is : 148
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 1: ordinal not in range(128)

visiting http://www.vice.com/dnd
	priority is -3
	the current length of the visited set is : 149
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 243: ordinal not in range(128)

visiting http://www.nationalgeographic.com/jobs/
	priority is 2
	the current length of the visited set is : 150
crawled http://www.nationalgeographic.com/jobs/ it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/news/calgary/
	priority is 2
	the current length of the visited set is : 151
crawled http://www.huffingtonpost.ca/news/calgary/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/05/20/hawaii-volcanoes-national-park-bioblitz-western-science-hawaiian-culture/
	priority is 2
	the current length of the visited set is : 152
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://www.vice.com/nsfw
	priority is -3
	the current length of the visited set is : 153
crawled http://www.vice.com/nsfw it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/about/
	priority is 2
	the current length of the visited set is : 154
crawled http://www.nationalgeographic.com/about/ it wasn't  news 
returning reqs
visiting http://nationalgeographic.us5.list-manage1.com/subscribe?u=8e3f01451c4d6bdc356c18ca0&id=4bf0325836
	priority is 0
	the current length of the visited set is : 155
crawled http://nationalgeographic.us5.list-manage1.com/subscribe?u=8e3f01451c4d6bdc356c18ca0&id=4bf0325836 it wasn't  news 
returning reqs
visiting https://books.google.ca/books
	priority is 2
	the current length of the visited set is : 156
crawled https://books.google.ca/books it wasn't  news 
returning reqs
visiting https://en.wikipedia.org/wiki/Genocide_Convention
	priority is 2
	the current length of the visited set is : 157
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xd7 in position 0: ordinal not in range(128)

visiting http://press.nationalgeographic.com/2015/05/15/national-geographic-magazine-june-2015/
	priority is 2
	the current length of the visited set is : 158
crawled http://press.nationalgeographic.com/2015/05/15/national-geographic-magazine-june-2015/ it was  news 
returning reqs
visiting http://www.vice.com/gaming
	priority is -3
	the current length of the visited set is : 159
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting https://www.documentcloud.org/documents/563338-fontaine-v-canada-2013-onsc-684.html
	priority is 2
	the current length of the visited set is : 160
crawled https://www.documentcloud.org/documents/563338-fontaine-v-canada-2013-onsc-684.html it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/teen/
	priority is 2
	the current length of the visited set is : 161
crawled http://www.huffingtonpost.com/teen/ it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/college/
	priority is 2
	the current length of the visited set is : 162
crawled http://www.huffingtonpost.com/college/ it wasn't  news 
returning reqs
visiting http://www.vice.com/food
	priority is -3
	the current length of the visited set is : 163
crawled http://www.vice.com/food it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/contact/
	priority is 2
	the current length of the visited set is : 164
crawled http://www.huffingtonpost.ca/contact/ it wasn't  news 
returning reqs
visiting http://www.engadget.com/
	priority is 2
	the current length of the visited set is : 165
crawled http://www.engadget.com/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/05/04/getting-your-shot-book/
	priority is 2
	the current length of the visited set is : 166
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting http://press.nationalgeographic.com/page/3/
	priority is 2
	the current length of the visited set is : 167
crawled http://press.nationalgeographic.com/page/3/ it was  news 
returning reqs
visiting http://adinfo.aol.com/about-our-ads/
	priority is 2
	the current length of the visited set is : 168
crawled http://adinfo.aol.com/about-our-ads/ it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/women/
	priority is 2
	the current length of the visited set is : 169
crawled http://www.huffingtonpost.com/women/ it wasn't  news 
returning reqs
visiting http://www.vice.com/tech
	priority is -3
	the current length of the visited set is : 170
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 3: ordinal not in range(128)

visiting http://www.huffingtonpost.ca/makehome/
	priority is 2
	the current length of the visited set is : 171
crawled http://www.huffingtonpost.ca/makehome/ it wasn't  news 
returning reqs
visiting http://www.amazon.com/War-Dogs-Canine-Heroism-History/dp/1137279680
	priority is 0
	the current length of the visited set is : 172
crawled http://www.amazon.com/War-Dogs-Canine-Heroism-History/dp/1137279680 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/tech/
	priority is 2
	the current length of the visited set is : 173
crawled http://www.huffingtonpost.com/tech/ it wasn't  news 
returning reqs
visiting http://www.vice.com/sports
	priority is -3
	the current length of the visited set is : 174
crawled http://www.vice.com/sports it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140518-dogs-war-canines-soldiers-troops-military-japanese-prisoner/
	priority is 2
	the current length of the visited set is : 175
crawled http://news.nationalgeographic.com/news/2014/05/140518-dogs-war-canines-soldiers-troops-military-japanese-prisoner/ it was  news 
returning reqs
visiting http://www.huffingtonpost.ca/feeds/verticals/canada-politics/index.xml
	priority is 2
	the current length of the visited set is : 176
crawled http://www.huffingtonpost.ca/feeds/verticals/canada-politics/index.xml it wasn't  news 
returning reqs
visiting http://www.engadget.com/gaming/
	priority is 0
	the current length of the visited set is : 177
crawled http://www.engadget.com/gaming/ it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/green/
	priority is 2
	the current length of the visited set is : 178
crawled http://www.huffingtonpost.com/green/ it wasn't  news 
returning reqs
visiting http://www.thecanadianpress.com/
	priority is 0
	the current length of the visited set is : 179
crawled http://www.thecanadianpress.com/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/05/08/national-geographic-studios-presents-robots-3d-narrated-by-simon-pegg-as-robothespian/
	priority is 2
	the current length of the visited set is : 180
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 15: ordinal not in range(128)

visiting http://www.theglobeandmail.com/news/national/atleo-hopes-un-visit-will-hold-a-mirror-to-gap-between-canada-and-first-nations/article14696108/
	priority is 2
	the current length of the visited set is : 181
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://news.nationalgeographic.com/news/2014/05/140517-dogs-war-canines-soldiers-troops-marine-military-pacific-japan/
	priority is 2
	the current length of the visited set is : 182
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)

visiting http://www.vice.com/travel
	priority is -3
	the current length of the visited set is : 183
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting http://www.huffingtonpost.com/own/
	priority is 2
	the current length of the visited set is : 184
crawled http://www.huffingtonpost.com/own/ it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/science/
	priority is 2
	the current length of the visited set is : 185
crawled http://www.huffingtonpost.com/science/ it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/feeds/verticals/canada/index.xml
	priority is 2
	the current length of the visited set is : 186
crawled http://www.huffingtonpost.ca/feeds/verticals/canada/index.xml it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/syndication/
	priority is 2
	the current length of the visited set is : 187
crawled http://www.huffingtonpost.ca/syndication/ it wasn't  news 
returning reqs
visiting http://unworthydominanceofregina.uregina.wikispaces.net/8+-+The+Story+of+Canada's+Indian+Residential+School+System?responseToken=0b41d67e757853ce910b33f6db227c930
	priority is 0
	the current length of the visited set is : 188
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://news.nationalgeographic.com/news/2014/05/140516-dogs-war-canines-soldiers-troops-army-military/
	priority is 2
	the current length of the visited set is : 189
crawled http://news.nationalgeographic.com/news/2014/05/140516-dogs-war-canines-soldiers-troops-army-military/ it wasn't  news 
returning reqs
visiting http://www.engadget.com/topics/apple/
	priority is 0
	the current length of the visited set is : 190
crawled http://www.engadget.com/topics/apple/ it wasn't  news 
returning reqs
visiting http://www.vice.com/fashion
	priority is -3
	the current length of the visited set is : 191
crawled http://www.vice.com/fashion it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/news/genocide-canada/
	priority is 2
	the current length of the visited set is : 192
crawled http://www.huffingtonpost.ca/news/genocide-canada/ it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/p/huffingtonpostca-about-us.html
	priority is 2
	the current length of the visited set is : 193
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 2: ordinal not in range(128)

visiting http://advertising.aol.com/properties
	priority is 0
	the current length of the visited set is : 194
crawled http://advertising.aol.com/properties it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/p/community-guidelines-ca.html
	priority is 2
	the current length of the visited set is : 195
crawled http://www.huffingtonpost.com/p/community-guidelines-ca.html it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/
	priority is 2
	the current length of the visited set is : 196
crawled http://news.nationalgeographic.com/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/05/11/hawaii-volcanoes-national-park-to-host-bioblitz-2015/
	priority is 2
	the current length of the visited set is : 197
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 16: ordinal not in range(128)

visiting http://www.vice.com/music
	priority is -3
	the current length of the visited set is : 198
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)

visiting http://america.aljazeera.com/articles/2013/10/14/un-declines-to-meetwithcanadianfirstnationamidrightstour.html
	priority is 2
	the current length of the visited set is : 199
crawled http://america.aljazeera.com/articles/2013/10/14/un-declines-to-meetwithcanadianfirstnationamidrightstour.html it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/news/united-nations-canada/
	priority is 2
	the current length of the visited set is : 200
crawled http://www.huffingtonpost.ca/news/united-nations-canada/ it wasn't  news 
returning reqs
visiting http://www.theglobeandmail.com/globe-debate/what-canada-committed-against-first-nations-was-genocide-the-un-should-recognize-it/article14853747/
	priority is 0
	the current length of the visited set is : 202
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)

visiting http://news.nationalgeographic.com/2015/05/150514-indiana-jones-archaeology-exhibit-national-geographic-museum/
	priority is 0
	the current length of the visited set is : 203
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://press.nationalgeographic.com/2015/05/11/advisory-10-students-qualify-for-2015-national-geographic-bee-championship-round-may-13/
	priority is 2
	the current length of the visited set is : 204
crawled http://press.nationalgeographic.com/2015/05/11/advisory-10-students-qualify-for-2015-national-geographic-bee-championship-round-may-13/ it was  news 
returning reqs
visiting http://www.vice.com/news
	priority is -3
	the current length of the visited set is : 205
crawled http://www.vice.com/news it wasn't  news 
returning reqs
visiting http://www.vice.com/videos
	priority is -3
	the current length of the visited set is : 206
crawled http://www.vice.com/videos it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/p/huffingtonpostca-privacy-policy.html
	priority is 2
	the current length of the visited set is : 207
crawled http://www.huffingtonpost.com/p/huffingtonpostca-privacy-policy.html it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/news/united-nations-canada-genocide/
	priority is 2
	the current length of the visited set is : 208
crawled http://www.huffingtonpost.ca/news/united-nations-canada-genocide/ it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/05/150518-cahokia-ancient-America-prehistoric-floods-mystery-Mississippi/
	priority is 0
	the current length of the visited set is : 209
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://www.huffingtonpost.com/p/huffingtonpostca-terms-and-con.html
	priority is 2
	the current length of the visited set is : 210
crawled http://www.huffingtonpost.com/p/huffingtonpostca-terms-and-con.html it wasn't  news 
returning reqs
visiting http://www.thestar.com/opinion/commentary/2013/07/19/a_canadian_genocide_in_search_of_a_name.html
	priority is 2
	the current length of the visited set is : 211
crawled http://www.thestar.com/opinion/commentary/2013/07/19/a_canadian_genocide_in_search_of_a_name.html it wasn't  news 
returning reqs
visiting http://www.vice.com/read/the-blobby-boys-meet-kiss-298
	priority is -3
	the current length of the visited set is : 212
crawled http://www.vice.com/read/the-blobby-boys-meet-kiss-298 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/news/genocide-natives/
	priority is 2
	the current length of the visited set is : 213
crawled http://www.huffingtonpost.ca/news/genocide-natives/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/tag/peter-essick/
	priority is 2
	the current length of the visited set is : 214
crawled http://press.nationalgeographic.com/tag/peter-essick/ it was  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/05/12/sedona-verde-valley-tourism-initiative/
	priority is 2
	the current length of the visited set is : 216
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 8: ordinal not in range(128)

visiting http://press.nationalgeographic.com/tag/nepal/
	priority is 2
	the current length of the visited set is : 217
crawled http://press.nationalgeographic.com/tag/nepal/ it was  news 
returning reqs
visiting http://press.nationalgeographic.com/tag/yoho-national-park/
	priority is 2
	the current length of the visited set is : 218
crawled http://press.nationalgeographic.com/tag/yoho-national-park/ it was  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/05/150516-syria-palmyra-islamic-state-ISIS-ISIL-ancient-Rome-destruction-world-heritage-archaeology/
	priority is 0
	the current length of the visited set is : 219
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 14: ordinal not in range(128)

visiting http://www.huffingtonpost.com/p/frequently-asked-question.html
	priority is 2
	the current length of the visited set is : 220
crawled http://www.huffingtonpost.com/p/frequently-asked-question.html it wasn't  news 
returning reqs
visiting http://www.vice.com/read/megg-mogg-owl-high-school-298
	priority is -3
	the current length of the visited set is : 221
crawled http://www.vice.com/read/megg-mogg-owl-high-school-298 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/news/idle-no-more/
	priority is 2
	the current length of the visited set is : 222
crawled http://www.huffingtonpost.ca/news/idle-no-more/ it wasn't  news 
returning reqs
visiting http://www.torontosun.com/2013/08/02/indian-genocide-thats-what-former-cjc-boss-bernie-farber-says-canada-is-guilty-of--a-bizarre-and-embarrassing-for-him-allegation
	priority is 2
	the current length of the visited set is : 223
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting http://www.theglobeandmail.com/globe-debate/when-canada-used-hunger-to-clear-the-west/article13316877/
	priority is 0
	the current length of the visited set is : 224
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://news.nationalgeographic.com/2015/04/150418-abraham-lincoln-funeral-train-railroad-civil-war-history/
	priority is 0
	the current length of the visited set is : 225
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://www.vice.com/read/michael-michael-fantasy
	priority is -3
	the current length of the visited set is : 226
crawled http://www.vice.com/read/michael-michael-fantasy it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/tag/hampton-sides/
	priority is 2
	the current length of the visited set is : 227
crawled http://press.nationalgeographic.com/tag/hampton-sides/ it was  news 
returning reqs
visiting http://www.bbc.com/news/world-11108059
	priority is 2
	the current length of the visited set is : 228
crawled http://www.bbc.com/news/world-11108059 it was  news 
returning reqs
visiting http://www.huffingtonpost.ca/news/genocide-aboriginals/
	priority is 2
	the current length of the visited set is : 229
crawled http://www.huffingtonpost.ca/news/genocide-aboriginals/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/page/4/
	priority is 2
	the current length of the visited set is : 231
crawled http://press.nationalgeographic.com/page/4/ it was  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/05/150508-wwii-planes-flyover-veterans-ve-day-victory-in-europe-video/
	priority is 0
	the current length of the visited set is : 232
crawled http://news.nationalgeographic.com/2015/05/150508-wwii-planes-flyover-veterans-ve-day-victory-in-europe-video/ it wasn't  news 
returning reqs
visiting http://www.pdsa.org.uk/what-we-do/animal-honours/pdsa-dickin-medal
	priority is 0
	the current length of the visited set is : 233
crawled http://www.pdsa.org.uk/what-we-do/animal-honours/pdsa-dickin-medal it wasn't  news 
returning reqs
visiting http://www.vice.com/read/blood-lady-commandos-orange-party-balls
	priority is -3
	the current length of the visited set is : 234
crawled http://www.vice.com/read/blood-lady-commandos-orange-party-balls it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/news/genocide-first-nations/
	priority is 2
	the current length of the visited set is : 235
crawled http://www.huffingtonpost.ca/news/genocide-first-nations/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/04/16/national-geographic-and-first-bankcard-launch-new-visa-card-with-exclusive-discounts-special-access-and-member-benefits/
	priority is 2
	the current length of the visited set is : 236
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://www.vice.com/tag/banksy
	priority is -3
	the current length of the visited set is : 237
crawled http://www.vice.com/tag/banksy it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/04/150415-ngbooktalk-nazis-auschwitz-holocaust-survivors/
	priority is 0
	the current length of the visited set is : 238
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting http://www.huffingtonpost.ca/news/genocide-canada-first-nations/
	priority is 2
	the current length of the visited set is : 239
crawled http://www.huffingtonpost.ca/news/genocide-canada-first-nations/ it wasn't  news 
returning reqs
visiting https://members.nationalgeographic.com/s/?next=/_membercenter/
	priority is 0
	the current length of the visited set is : 240
crawled https://members.nationalgeographic.com/s/?next=/_membercenter/ it wasn't  news 
returning reqs
visiting http://www.ottawacitizen.com/opinion/op-ed/troubling+legacy+Duncan+Campbell+Scott/8844210/story.html
	priority is 2
	the current length of the visited set is : 241
crawled http://www.ottawacitizen.com/opinion/op-ed/troubling+legacy+Duncan+Campbell+Scott/8844210/story.html it wasn't  news 
returning reqs
visiting https://twitter.com/michaelbolen
	priority is 0
	the current length of the visited set is : 242
crawled https://twitter.com/michaelbolen it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/Rob+Corradetti
	priority is -3
	the current length of the visited set is : 243
crawled http://www.vice.com/tag/Rob+Corradetti it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/news/genocide-un-canada/
	priority is 2
	the current length of the visited set is : 244
crawled http://www.huffingtonpost.ca/news/genocide-un-canada/ it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/news/genocide-canada-natives/
	priority is 2
	the current length of the visited set is : 245
crawled http://www.huffingtonpost.ca/news/genocide-canada-natives/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/tag/geography/
	priority is 2
	the current length of the visited set is : 246
crawled http://press.nationalgeographic.com/tag/geography/ it was  news 
returning reqs
visiting https://ca.news.yahoo.com/blogs/canada-politics/former-afn-chief-wants-un-recognize-canada-treatment-182209449.html
	priority is 0
	the current length of the visited set is : 247
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 15: ordinal not in range(128)

visiting http://press.nationalgeographic.com/tag/bee/
	priority is 2
	the current length of the visited set is : 248
crawled http://press.nationalgeographic.com/tag/bee/ it was  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/04/28/geography-whiz-kids-to-vie-for-2015-national-geographic-bee-champion-title-and-85000-in-college-scholarships/
	priority is 2
	the current length of the visited set is : 249
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 1: ordinal not in range(128)

visiting http://www.vice.com/tag/artists
	priority is -3
	the current length of the visited set is : 250
crawled http://www.vice.com/tag/artists it wasn't  news 
returning reqs
visiting http://www.parl.gc.ca/Default.aspx
	priority is 2
	the current length of the visited set is : 251
crawled http://www.parl.gc.ca/Default.aspx it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140516-wolves-oregon-or7-science-endangered-species-animals/
	priority is 0
	the current length of the visited set is : 252
crawled http://news.nationalgeographic.com/news/2014/05/140516-wolves-oregon-or7-science-endangered-species-animals/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/03/16/national-geographic-magazine-april-2015/
	priority is 2
	the current length of the visited set is : 253
crawled http://press.nationalgeographic.com/2015/03/16/national-geographic-magazine-april-2015/ it was  news 
returning reqs
visiting http://www.vice.com/tag/VICE+Comics
	priority is -3
	the current length of the visited set is : 254
crawled http://www.vice.com/tag/VICE+Comics it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2013/10/18/rexton-protest-nb-photos_n_4121035.html
	priority is 2
	the current length of the visited set is : 255
crawled http://www.huffingtonpost.ca/2013/10/18/rexton-protest-nb-photos_n_4121035.html it was  news 
returning reqs
visiting http://www.vice.com/tag/comics
	priority is -3
	the current length of the visited set is : 256
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)

visiting https://www.dropbox.com/sh/21p0ve7oim9rbd1/AADdjbhTPysM9SdqnBiBk6F0a
	priority is 2
	the current length of the visited set is : 257
crawled https://www.dropbox.com/sh/21p0ve7oim9rbd1/AADdjbhTPysM9SdqnBiBk6F0a it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2013/10/17/elsipogtog-photos-rcmp-protest-violence_n_4114506.html
	priority is 2
	the current length of the visited set is : 258
crawled http://www.huffingtonpost.ca/2013/10/17/elsipogtog-photos-rcmp-protest-violence_n_4114506.html it was  news 
returning reqs
visiting http://press.nationalgeographic.com/tag/peter-essick/feed/
	priority is 2
	the current length of the visited set is : 259
crawled http://press.nationalgeographic.com/tag/peter-essick/feed/ it wasn't  news 
returning reqs
visiting http://www.vice.com/read/artists-of-today-000
	priority is -3
	the current length of the visited set is : 260
crawled http://www.vice.com/read/artists-of-today-000 it wasn't  news 
returning reqs
visiting http://www.bbc.com/privacy/
	priority is 2
	the current length of the visited set is : 261
crawled http://www.bbc.com/privacy/ it wasn't  news 
returning reqs
visiting http://www.bbc.com/privacy/cookies/about
	priority is 2
	the current length of the visited set is : 262
crawled http://www.bbc.com/privacy/cookies/about it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/tag/yoho-national-park/feed/
	priority is 2
	the current length of the visited set is : 263
crawled http://press.nationalgeographic.com/tag/yoho-national-park/feed/ it wasn't  news 
returning reqs
visiting http://www.bbc.co.uk/news/20039682
	priority is 2
	the current length of the visited set is : 264
crawled http://www.bbc.co.uk/news/20039682 it wasn't  news 
returning reqs
visiting http://www.vice.com/series/comics
	priority is -3
	the current length of the visited set is : 265
crawled http://www.vice.com/series/comics it wasn't  news 
returning reqs
visiting http://www.bbc.co.uk/news/10628323
	priority is 2
	the current length of the visited set is : 266
crawled http://www.bbc.co.uk/news/10628323 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2012/02/17/residential-schools-canada-genocide_n_1285371.html
	priority is 2
	the current length of the visited set is : 267
crawled http://www.huffingtonpost.ca/2012/02/17/residential-schools-canada-genocide_n_1285371.html it was  news 
returning reqs
visiting https://careers.smartrecruiters.com/Aol/huffington-post/
	priority is 0
	the current length of the visited set is : 268
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)

visiting https://itunes.apple.com/us/app/the-huffington-post/id306621789
	priority is 2
	the current length of the visited set is : 269
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting http://press.nationalgeographic.com/page/5/
	priority is 2
	the current length of the visited set is : 270
crawled http://press.nationalgeographic.com/page/5/ it was  news 
returning reqs
visiting https://www.icrc.org/applic/ihl/ihl.nsf/Treaty.xsp
	priority is 0
	the current length of the visited set is : 271
crawled https://www.icrc.org/applic/ihl/ihl.nsf/Treaty.xsp it wasn't  news 
returning reqs
visiting http://www.vice.com/articles/page/1486
	priority is -3
	the current length of the visited set is : 272
crawled http://www.vice.com/articles/page/1486 it wasn't  news 
returning reqs
visiting https://itunes.apple.com/us/app/huffpost-live/id572584499
	priority is 2
	the current length of the visited set is : 273
crawled https://itunes.apple.com/us/app/huffpost-live/id572584499 it wasn't  news 
returning reqs
visiting http://www.bbc.co.uk/news/help-17655000
	priority is 2
	the current length of the visited set is : 274
crawled http://www.bbc.co.uk/news/help-17655000 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2013/04/26/canada-residential-schools-paul-martin_n_3167061.html
	priority is 2
	the current length of the visited set is : 275
crawled http://www.huffingtonpost.ca/2013/04/26/canada-residential-schools-paul-martin_n_3167061.html it was  news 
returning reqs
visiting http://patch.com/
	priority is 0
	the current length of the visited set is : 276
crawled http://patch.com/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/03/31/jaipur-rugs-to-produce-new-line-of-rugs-for-national-geographic/
	priority is 2
	the current length of the visited set is : 277
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://www.theglobeandmail.com/news/national/rcmp-move-in-on-first-nation-protesting-shale-gas-development/article14904344/
	priority is 2
	the current length of the visited set is : 278
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://www.huffingtonpost.ca/2013/10/16/throne-speech-highlights-2013_n_4110961.html
	priority is 2
	the current length of the visited set is : 279
crawled http://www.huffingtonpost.ca/2013/10/16/throne-speech-highlights-2013_n_4110961.html it was  news 
returning reqs
visiting http://www.bbc.co.uk/news/10628994
	priority is 2
	the current length of the visited set is : 280
crawled http://www.bbc.co.uk/news/10628994 it wasn't  news 
returning reqs
visiting http://www.theglobeandmail.com/news/politics/we-are-all-responsible-for-the-plight-of-canadas-first-nations/article2341840/
	priority is 2
	the current length of the visited set is : 284
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://www.bbc.com/news/business-12686570
	priority is 2
	the current length of the visited set is : 285
crawled http://www.bbc.com/news/business-12686570 it wasn't  news 
returning reqs
visiting http://www.bbc.co.uk/help/web/links/
	priority is 0
	the current length of the visited set is : 286
crawled http://www.bbc.co.uk/help/web/links/ it wasn't  news 
returning reqs
visiting https://twitter.com/01LBrown/statuses/390891610755461120
	priority is 2
	the current length of the visited set is : 287
crawled https://twitter.com/01LBrown/statuses/390891610755461120 it wasn't  news 
returning reqs
visiting http://www.vice.com/articles/page/5
	priority is -3
	the current length of the visited set is : 288
crawled http://www.vice.com/articles/page/5 it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/04/07/national-geographic-traveler-magazine-launches-2015-photography-contest/
	priority is 2
	the current length of the visited set is : 289
crawled http://press.nationalgeographic.com/2015/04/07/national-geographic-traveler-magazine-launches-2015-photography-contest/ it was  news 
returning reqs
visiting http://www.huffingtonpost.ca/2013/10/15/james-anaya-united-nations-inquiry_n_4101817.html
	priority is 2
	the current length of the visited set is : 290
crawled http://www.huffingtonpost.ca/2013/10/15/james-anaya-united-nations-inquiry_n_4101817.html it was  news 
returning reqs
visiting http://rabble.ca/blogs/bloggers/krystalline-kraus/2013/10/activist-communique-news-release-elsipogtog-first-nations-s
	priority is 2
	the current length of the visited set is : 291
crawled http://rabble.ca/blogs/bloggers/krystalline-kraus/2013/10/activist-communique-news-release-elsipogtog-first-nations-s it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/04/09/traveler-magazine-selects-2015-tours-of-a-lifetime/
	priority is 2
	the current length of the visited set is : 292
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting https://www.pinterest.com/join/?next=/pin/create/button/
	priority is 0
	the current length of the visited set is : 293
crawled https://www.pinterest.com/join/?next=/pin/create/button/ it wasn't  news 
returning reqs
visiting http://www.vice.com/articles/page/4
	priority is -3
	the current length of the visited set is : 294
crawled http://www.vice.com/articles/page/4 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2013/07/16/aboriginal-nutritional-experiment_n_3605503.html
	priority is 2
	the current length of the visited set is : 295
crawled http://www.huffingtonpost.ca/2013/07/16/aboriginal-nutritional-experiment_n_3605503.html it was  news 
returning reqs
visiting http://halifax.mediacoop.ca/video/warrior-society-call-support-elsipogtog-seizure-fr/19272
	priority is 2
	the current length of the visited set is : 297
crawled http://halifax.mediacoop.ca/video/warrior-society-call-support-elsipogtog-seizure-fr/19272 it wasn't  news 
returning reqs
visiting http://www.leaderpost.com/technology/RCMP+Brunswick+enforce+injunction+against+shale+protesters/9047164/story.html
	priority is 2
	the current length of the visited set is : 298
crawled http://www.leaderpost.com/technology/RCMP+Brunswick+enforce+injunction+against+shale+protesters/9047164/story.html it wasn't  news 
returning reqs
visiting http://halifax.mediacoop.ca/story/injunction-against-anti-shale-gas-activists-issued/19111
	priority is 2
	the current length of the visited set is : 299
crawled http://halifax.mediacoop.ca/story/injunction-against-anti-shale-gas-activists-issued/19111 it wasn't  news 
returning reqs
visiting http://www.vice.com/articles/page/3
	priority is -3
	the current length of the visited set is : 300
crawled http://www.vice.com/articles/page/3 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/bernie-farber/canada-genocide-first-nations_b_4122651.html
	priority is 2
	the current length of the visited set is : 301
crawled http://www.huffingtonpost.ca/bernie-farber/canada-genocide-first-nations_b_4122651.html it was  news 
returning reqs
visiting http://www.bbc.com/news/business-11428889
	priority is 2
	the current length of the visited set is : 302
crawled http://www.bbc.com/news/business-11428889 it wasn't  news 
returning reqs
visiting http://www.bbc.com/news/business/business_of_sport
	priority is 2
	the current length of the visited set is : 305
crawled http://www.bbc.com/news/business/business_of_sport it wasn't  news 
returning reqs
visiting https://twitter.com/SettlerColonial?protected_redirect=true
	priority is 0
	the current length of the visited set is : 306
crawled https://twitter.com/SettlerColonial?protected_redirect=true it wasn't  news 
returning reqs
visiting http://www.vice.com/articles/page/2
	priority is -3
	the current length of the visited set is : 307
crawled http://www.vice.com/articles/page/2 it wasn't  news 
returning reqs
visiting http://www.thestar.com/news/canada/2013/10/15/un_investigator_urges_inquiry_for_missing_murdered_aboriginal_women.html
	priority is 2
	the current length of the visited set is : 308
crawled http://www.thestar.com/news/canada/2013/10/15/un_investigator_urges_inquiry_for_missing_murdered_aboriginal_women.html it wasn't  news 
returning reqs
visiting http://www.winnipegfreepress.com/canada/canadas-residential-schools-a-form-of-genocide-truth-and-reconciliation-chair-139536198.html
	priority is 2
	the current length of the visited set is : 310
crawled http://www.winnipegfreepress.com/canada/canadas-residential-schools-a-form-of-genocide-truth-and-reconciliation-chair-139536198.html it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2013/02/28/geography-legislators-of-the-year/
	priority is 2
	the current length of the visited set is : 311
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 8: ordinal not in range(128)

visiting http://www.presstv.ir/detail/2013/04/26/300307/canadian-aboriginals-remember-childhood-abuses/
	priority is 2
	the current length of the visited set is : 312
crawled http://www.presstv.ir/detail/2013/04/26/300307/canadian-aboriginals-remember-childhood-abuses/ it wasn't  news 
returning reqs
visiting https://www.facebook.com/AlthiaRaj
	priority is 0
	the current length of the visited set is : 313
crawled https://www.facebook.com/AlthiaRaj it wasn't  news 
returning reqs
visiting http://www.vancouversun.com/news/Government+still+trying+assimilate+aboriginals+underfunding+child+welfare+natives+argue/6153363/story.html
	priority is 2
	the current length of the visited set is : 314
crawled http://www.vancouversun.com/news/Government+still+trying+assimilate+aboriginals+underfunding+child+welfare+natives+argue/6153363/story.html it wasn't  news 
returning reqs
visiting http://www.theglobeandmail.com/report-on-business/ottawa-steps-up-efforts-to-battle-climate-change/article13235015/
	priority is 2
	the current length of the visited set is : 315
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)

visiting http://www.vice.com/articles/page/1
	priority is -3
	the current length of the visited set is : 316
crawled http://www.vice.com/articles/page/1 it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/04/15/national-geographic-magazine-may-2015/
	priority is 2
	the current length of the visited set is : 317
crawled http://press.nationalgeographic.com/2015/04/15/national-geographic-magazine-may-2015/ it was  news 
returning reqs
visiting http://www.vice.com/read/proudtobeirish-trending-as-ireland-votes-yes-to-marriage-equality-235
	priority is -3
	the current length of the visited set is : 318
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting http://www.montrealgazette.com/Opinion+Residential+schools+part+every+Canadian+history/8263739/story.html
	priority is 2
	the current length of the visited set is : 319
crawled http://www.montrealgazette.com/Opinion+Residential+schools+part+every+Canadian+history/8263739/story.html it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/michael-bolen/
	priority is 2
	the current length of the visited set is : 320
crawled http://www.huffingtonpost.ca/michael-bolen/ it was  news 
returning reqs
visiting http://www.theglobeandmail.com/news/politics/competing-aboriginal-meetings-hint-at-schism-within-first-nations-community/article13215473/
	priority is 2
	the current length of the visited set is : 323
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 27: ordinal not in range(128)

visiting http://www.ctvnews.ca/canada/canada-faces-a-crisis-on-aboriginal-reserves-un-investigator-1.1497612
	priority is 2
	the current length of the visited set is : 324
crawled http://www.ctvnews.ca/canada/canada-faces-a-crisis-on-aboriginal-reserves-un-investigator-1.1497612 it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/tag/geography/page/2/
	priority is 2
	the current length of the visited set is : 325
crawled http://press.nationalgeographic.com/tag/geography/page/2/ it was  news 
returning reqs
visiting http://www2.canada.com/residential+school+hearings+help+healing+alphonse/6155647/story.html?id=6155647
	priority is 0
	the current length of the visited set is : 326
crawled http://www2.canada.com/residential+school+hearings+help+healing+alphonse/6155647/story.html?id=6155647 it wasn't  news 
returning reqs
visiting http://www.bbc.com/news/business-22434141
	priority is 2
	the current length of the visited set is : 327
crawled http://www.bbc.com/news/business-22434141 it wasn't  news 
returning reqs
visiting http://atlantic.ctvnews.ca/rcmp-vehicles-on-fire-at-n-b-shale-gas-protest-1.1500862
	priority is 0
	the current length of the visited set is : 328
crawled http://atlantic.ctvnews.ca/rcmp-vehicles-on-fire-at-n-b-shale-gas-protest-1.1500862 it wasn't  news 
returning reqs
visiting http://www.ctvnews.ca/canada/molotov-cocktails-thrown-as-shale-gas-protesters-clash-with-rcmp-in-eastern-n-b-1.1500986
	priority is 0
	the current length of the visited set is : 329
crawled http://www.ctvnews.ca/canada/molotov-cocktails-thrown-as-shale-gas-protesters-clash-with-rcmp-in-eastern-n-b-1.1500986 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2015/05/21/conservative-debate-dance_n_7368146.html
	priority is 2
	the current length of the visited set is : 330
crawled http://www.huffingtonpost.ca/2015/05/21/conservative-debate-dance_n_7368146.html it was  news 
returning reqs
visiting http://www.ctvnews.ca/politics/throne-speech-government-pledges-law-for-balanced-budgets-1.1499045
	priority is 0
	the current length of the visited set is : 333
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)

visiting http://press.nationalgeographic.com/2013/07/29/media-advisory/
	priority is 2
	the current length of the visited set is : 334
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 17: ordinal not in range(128)

visiting http://www.msn.com/en-ca/news
	priority is 0
	the current length of the visited set is : 335
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)

visiting http://www.theglobeandmail.com/news/politics/first-nations-chiefs-hold-dissenting-gathering-as-atleos-leadership-is-under-scrutiny/article13238489/
	priority is 2
	the current length of the visited set is : 336
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 8: ordinal not in range(128)

visiting http://www.presstv.ir/detail/2013/06/22/310340/indigenous-canadians-mark-national-aboriginal-day/
	priority is 2
	the current length of the visited set is : 337
crawled http://www.presstv.ir/detail/2013/06/22/310340/indigenous-canadians-mark-national-aboriginal-day/ it wasn't  news 
returning reqs
visiting http://www.vice.com/read/yes-you-actually-can-shatter-your-dick-253
	priority is -3
	the current length of the visited set is : 338
crawled http://www.vice.com/read/yes-you-actually-can-shatter-your-dick-253 it was  news 
returning reqs
visiting http://www.bbc.co.uk/news/business/market_data/overview/
	priority is 0
	the current length of the visited set is : 339
crawled http://www.bbc.co.uk/news/business/market_data/overview/ it wasn't  news 
returning reqs
visiting https://twitter.com/GeorgieBC/statuses/390877972925657088
	priority is 2
	the current length of the visited set is : 340
crawled https://twitter.com/GeorgieBC/statuses/390877972925657088 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2015/05/21/pei-cabinet-minister-r_n_7412746.html
	priority is 2
	the current length of the visited set is : 341
crawled http://www.huffingtonpost.ca/2015/05/21/pei-cabinet-minister-r_n_7412746.html it was  news 
returning reqs
visiting http://www.mississauga.com/news-story/4158674-is-government-trying-to-reshape-cdn-history-/
	priority is 2
	the current length of the visited set is : 342
crawled http://www.mississauga.com/news-story/4158674-is-government-trying-to-reshape-cdn-history-/ it was  news 
returning reqs
visiting https://twitter.com/INMvmt/statuses/390874486847062016
	priority is 2
	the current length of the visited set is : 343
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 23: ordinal not in range(128)

visiting http://www.durhamregion.com/news-story/4155812-throne-speech-to-rev-up-tory-campaign-engine/
	priority is 2
	the current length of the visited set is : 344
crawled http://www.durhamregion.com/news-story/4155812-throne-speech-to-rev-up-tory-campaign-engine/ it wasn't  news 
returning reqs
visiting https://www.facebook.com/alexandros.cons
	priority is 2
	the current length of the visited set is : 345
crawled https://www.facebook.com/alexandros.cons it wasn't  news 
returning reqs
visiting http://www.bbc.com/news/business/economy
	priority is 2
	the current length of the visited set is : 346
crawled http://www.bbc.com/news/business/economy it wasn't  news 
returning reqs
visiting http://www.vice.com/read/how-will-cleveland-respond-to-the-michael-brelo-verdict-523
	priority is -3
	the current length of the visited set is : 347
crawled http://www.vice.com/read/how-will-cleveland-respond-to-the-michael-brelo-verdict-523 it was  news 
returning reqs
visiting http://www.vice.com/tag/v22n5
	priority is 0
	the current length of the visited set is : 348
crawled http://www.vice.com/tag/v22n5 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2015/05/21/pierre-poilievre-liberal-video-ad_n_7413902.html
	priority is 2
	the current length of the visited set is : 349
crawled http://www.huffingtonpost.ca/2015/05/21/pierre-poilievre-liberal-video-ad_n_7413902.html it was  news 
returning reqs
visiting http://www.huffingtonpost.com/jumoke-balogun/baga-attack_b_6471254.html
	priority is 2
	the current length of the visited set is : 350
crawled http://www.huffingtonpost.com/jumoke-balogun/baga-attack_b_6471254.html it was  news 
returning reqs
visiting http://press.nationalgeographic.com/tag/geography/feed/
	priority is 2
	the current length of the visited set is : 352
crawled http://press.nationalgeographic.com/tag/geography/feed/ it wasn't  news 
returning reqs
visiting https://twitter.com/ndnchick1
	priority is 2
	the current length of the visited set is : 353
crawled https://twitter.com/ndnchick1 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2015/05/22/mackay-says-20-million-t_n_7425058.html
	priority is 2
	the current length of the visited set is : 354
crawled http://www.huffingtonpost.ca/2015/05/22/mackay-says-20-million-t_n_7425058.html it was  news 
returning reqs
visiting http://www.nationalgeographicexpeditions.com/expeditions/costa-rica-photo-tour/detail
	priority is 2
	the current length of the visited set is : 355
crawled http://www.nationalgeographicexpeditions.com/expeditions/costa-rica-photo-tour/detail it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/The+Making+of+Zombie+Wars:+A+Novel
	priority is 0
	the current length of the visited set is : 356
crawled http://www.vice.com/tag/The+Making+of+Zombie+Wars:+A+Novel it wasn't  news 
returning reqs
visiting http://www.trurodaily.com/News/Canada%20-%20World/2013-07-15/article-3315070/Competing-aboriginal-meetings-hint-at-schism-within-First-Nations-community/1
	priority is 2
	the current length of the visited set is : 357
crawled http://www.trurodaily.com/News/Canada%20-%20World/2013-07-15/article-3315070/Competing-aboriginal-meetings-hint-at-schism-within-First-Nations-community/1 it wasn't  news 
returning reqs
visiting https://twitter.com/01LBrown/statuses/390886965635080193
	priority is 2
	the current length of the visited set is : 361
crawled https://twitter.com/01LBrown/statuses/390886965635080193 it wasn't  news 
returning reqs
visiting http://travel.nationalgeographic.com/photo-contest-2015/rules
	priority is 2
	the current length of the visited set is : 362
crawled http://travel.nationalgeographic.com/photo-contest-2015/rules it wasn't  news 
returning reqs
visiting http://www.schoonerheritage.com/
	priority is 2
	the current length of the visited set is : 363
crawled http://www.schoonerheritage.com/ it wasn't  news 
returning reqs
visiting http://www.theglobeandmail.com/globe-debate/first-nations-should-realize-the-afn-backs-our-treaty-ambitions/article13240609/
	priority is 0
	the current length of the visited set is : 364
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 27: ordinal not in range(128)

visiting http://www.princegeorgecitizen.com/throne-speech-sets-priorities-1.1033287
	priority is 0
	the current length of the visited set is : 365
crawled http://www.princegeorgecitizen.com/throne-speech-sets-priorities-1.1033287 it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/2014/11/13/geography-awareness-week-2014/
	priority is 2
	the current length of the visited set is : 366
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://www.bhphotovideo.com/
	priority is 2
	the current length of the visited set is : 367
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 163, in goahead
    k = self.parse_endtag(i)
  File "C:\Python27\lib\HTMLParser.py", line 401, in parse_endtag
    self.handle_endtag(elem)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 34, in handle_endtag
    if tag == self.tag_stack[-1]:
IndexError: list index out of range

visiting http://www.huffingtonpost.ca/2015/05/21/legal-experts-decry-retro_n_7408530.html
	priority is 2
	the current length of the visited set is : 368
crawled http://www.huffingtonpost.ca/2015/05/21/legal-experts-decry-retro_n_7408530.html it was  news 
returning reqs
visiting http://www.liveleak.com/view
	priority is 2
	the current length of the visited set is : 369
crawled http://www.liveleak.com/view it wasn't  news 
returning reqs
visiting http://www.canada150.gc.ca/eng/1407417717020
	priority is 2
	the current length of the visited set is : 370
crawled http://www.canada150.gc.ca/eng/1407417717020 it wasn't  news 
returning reqs
visiting http://espn.go.com/espn/story/_/page/instantawesome-141104/this-curling-shot-definitely-best-curling-shot-ever-seen
	priority is 2
	the current length of the visited set is : 371
crawled http://espn.go.com/espn/story/_/page/instantawesome-141104/this-curling-shot-definitely-best-curling-shot-ever-seen it wasn't  news 
returning reqs
visiting https://twitter.com/norm/status/555080851760111616
	priority is 2
	the current length of the visited set is : 372
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://www.huffingtonpost.ca/2015/05/22/pre-election-quebec-push-_n_7419166.html
	priority is 2
	the current length of the visited set is : 373
crawled http://www.huffingtonpost.ca/2015/05/22/pre-election-quebec-push-_n_7419166.html it was  news 
returning reqs
visiting http://press.nationalgeographic.com/tag/bee/feed/
	priority is 2
	the current length of the visited set is : 375
crawled http://press.nationalgeographic.com/tag/bee/feed/ it wasn't  news 
returning reqs
visiting http://www.rgd.ca/2015/01/13/my-time-has-value.php
	priority is 2
	the current length of the visited set is : 376
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 8: ordinal not in range(128)

visiting https://twitter.com/CBCBlairRhodes/status/555744348470788098
	priority is 2
	the current length of the visited set is : 377
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting https://plus.google.com/110841765762299577555/posts
	priority is 2
	the current length of the visited set is : 378
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 6: ordinal not in range(128)

visiting http://www.huffingtonpost.ca/2015/05/22/andrew-kania-brampton-election-conservatives_n_7421450.html
	priority is 2
	the current length of the visited set is : 379
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://press.nationalgeographic.com/2015/03/23/national-geographic-photo-camp-to-mentor-young-syrian-refugees-in-jerash-jordan/
	priority is 2
	the current length of the visited set is : 380
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://press.nationalgeographic.com/page/6/
	priority is 2
	the current length of the visited set is : 383
crawled http://press.nationalgeographic.com/page/6/ it was  news 
returning reqs
visiting http://www.theguardian.pe.ca/News/Local/2015-03-05/article-4067050/Former-Summerside-councillor-Tina-Mundy-wins-Liberal-nomination/1
	priority is 2
	the current length of the visited set is : 384
crawled http://www.theguardian.pe.ca/News/Local/2015-03-05/article-4067050/Former-Summerside-councillor-Tina-Mundy-wins-Liberal-nomination/1 it wasn't  news 
returning reqs
visiting http://blogs.webmd.com/womens-health/2012/05/oh-no-where-did-it-go-when-things-get-lost-in-the-vagina.html
	priority is 2
	the current length of the visited set is : 385
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)

visiting http://www.ctvnews.ca/canada/scc-rules-retroactive-parole-law-unconstitutional-1.1737966
	priority is 2
	the current length of the visited set is : 386
crawled http://www.ctvnews.ca/canada/scc-rules-retroactive-parole-law-unconstitutional-1.1737966 it wasn't  news 
returning reqs
visiting http://www.bbc.co.uk/news/world-africa-30794829
	priority is 2
	the current length of the visited set is : 387
crawled http://www.bbc.co.uk/news/world-africa-30794829 it was  news 
returning reqs
visiting http://www.thefrisky.com/2011-06-28/8-sex-injuries/
	priority is 2
	the current length of the visited set is : 388
crawled http://www.thefrisky.com/2011-06-28/8-sex-injuries/ it wasn't  news 
returning reqs
visiting http://www.punchng.com/entertainment/sex-sexuality/common-sex-related-injuries-and-their-cures-i/
	priority is 2
	the current length of the visited set is : 389
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)

visiting http://www.fitnessrepublic.com/inspiration/lifestyle/6-common-sex-injuries-and-how-to-deal.html
	priority is 2
	the current length of the visited set is : 390
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting https://twitter.com/CBCRadioCanada
	priority is 2
	the current length of the visited set is : 391
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 1: ordinal not in range(128)

visiting http://www.bbc.co.uk/aboutthebbc/
	priority is 0
	the current length of the visited set is : 392
crawled http://www.bbc.co.uk/aboutthebbc/ it wasn't  news 
returning reqs
visiting http://www.bbc.co.uk/iplayer/guidance
	priority is 0
	the current length of the visited set is : 393
crawled http://www.bbc.co.uk/iplayer/guidance it wasn't  news 
returning reqs
visiting http://www.threehundredeight.com/p/canada.html
	priority is 2
	the current length of the visited set is : 394
crawled http://www.threehundredeight.com/p/canada.html it wasn't  news 
returning reqs
visiting http://www.webmd.com/a-to-z-guides/understanding-bladder-infections-basic-information
	priority is 2
	the current length of the visited set is : 395
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting http://www.huffingtonpost.ca/2015/05/22/ambrose-unveils-details-o_n_7424642.html
	priority is 2
	the current length of the visited set is : 396
crawled http://www.huffingtonpost.ca/2015/05/22/ambrose-unveils-details-o_n_7424642.html it was  news 
returning reqs
visiting http://www.electionalmanac.com/ea/canada/
	priority is 2
	the current length of the visited set is : 397
crawled http://www.electionalmanac.com/ea/canada/ it was  news 
returning reqs
visiting http://press.nationalgeographic.com/2015/03/31/national-geographic-travel-launches-first-interactive-digital-map-showcasing-switzerlands-grand-tour/
	priority is 2
	the current length of the visited set is : 405
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 8: ordinal not in range(128)

visiting http://wherearethechildren.ca/blackboard/page-7.html
	priority is 0
	the current length of the visited set is : 406
crawled http://wherearethechildren.ca/blackboard/page-7.html it wasn't  news 
returning reqs
visiting https://twitter.com/jmcguirecbc
	priority is 2
	the current length of the visited set is : 407
crawled https://twitter.com/jmcguirecbc it wasn't  news 
returning reqs
visiting http://edition.cnn.com/2015/01/09/africa/boko-haram-violence/
	priority is 2
	the current length of the visited set is : 408
crawled http://edition.cnn.com/2015/01/09/africa/boko-haram-violence/ it was  news 
returning reqs
visiting http://www.msnbc.com/msnbc/world-asks-wheres-the-global-outcry-victims-boko-haram-attacks
	priority is 2
	the current length of the visited set is : 409
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 14: ordinal not in range(128)

visiting http://www.washingtonpost.com/blogs/wonkblog/wp/2015/02/10/sex-toy-injuries-surged-after-fifty-shades-of-grey-was-published/
	priority is 0
	the current length of the visited set is : 410
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 6: ordinal not in range(128)

visiting http://www.ctvnews.ca/politics/four-reasons-why-canadians-could-head-to-the-polls-early-1.2188079
	priority is 2
	the current length of the visited set is : 411
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)

visiting http://www.independent.co.uk/news/world/africa/nigerian-archbishop-calls-for-same-spirit-of-support-as-france-after-2000-killed-by-boko-haram-in-baga-attack-9975390.html
	priority is 2
	the current length of the visited set is : 412
crawled http://www.independent.co.uk/news/world/africa/nigerian-archbishop-calls-for-same-spirit-of-support-as-france-after-2000-killed-by-boko-haram-in-baga-attack-9975390.html it wasn't  news 
returning reqs
visiting http://www.bbc.co.uk/contact
	priority is 0
	the current length of the visited set is : 413
crawled http://www.bbc.co.uk/contact it wasn't  news 
returning reqs
visiting http://www.smh.com.au/world/nigeria-needs-same-support-as-france-after-boko-haram-attacks-kill-over-2000-archbishop-20150112-12msu6.html
	priority is 2
	the current length of the visited set is : 414
crawled http://www.smh.com.au/world/nigeria-needs-same-support-as-france-after-boko-haram-attacks-kill-over-2000-archbishop-20150112-12msu6.html it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2015/05/19/canada-dictatorship-united-states-teens-survey_n_7330444.html
	priority is 2
	the current length of the visited set is : 415
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 9: ordinal not in range(128)

visiting http://www.metroland.com/
	priority is 2
	the current length of the visited set is : 420
crawled http://www.metroland.com/ it wasn't  news 
returning reqs
visiting https://twitter.com/@berniefarber
	priority is 0
	the current length of the visited set is : 421
crawled https://twitter.com/@berniefarber it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2015/05/21/leaders-debates-ndp-universities-neutral_n_7417218.html
	priority is 2
	the current length of the visited set is : 422
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)

visiting http://www.cnn.com/2015/01/12/africa/boko-haram-deadliest-attack/
	priority is 2
	the current length of the visited set is : 423
crawled http://www.cnn.com/2015/01/12/africa/boko-haram-deadliest-attack/ it was  news 
returning reqs
visiting https://www.facebook.com/humansofnewyork
	priority is 2
	the current length of the visited set is : 424
crawled https://www.facebook.com/humansofnewyork it wasn't  news 
returning reqs
visiting https://twitter.com/goraladka
	priority is 2
	the current length of the visited set is : 425
crawled https://twitter.com/goraladka it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2015/05/22/federal-ndp-poll-mulcair-ekos-tories-liberals_n_7422800.html
	priority is 2
	the current length of the visited set is : 426
crawled http://www.huffingtonpost.ca/2015/05/22/federal-ndp-poll-mulcair-ekos-tories-liberals_n_7422800.html it wasn't  news 
returning reqs
visiting http://www.metroland.com/privacy-policy
	priority is 0
	the current length of the visited set is : 428
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)

visiting http://press.nationalgeographic.com/tag/photo-contest/
	priority is 2
	the current length of the visited set is : 429
crawled http://press.nationalgeographic.com/tag/photo-contest/ it was  news 
returning reqs
visiting http://autocatch.com/mississauga-ontario/used-cars.htm
	priority is 2
	the current length of the visited set is : 430
crawled http://autocatch.com/mississauga-ontario/used-cars.htm it wasn't  news 
returning reqs
visiting http://www.goldbook.ca/
	priority is 2
	the current length of the visited set is : 431
crawled http://www.goldbook.ca/ it wasn't  news 
returning reqs
visiting http://www.metroland.com/newspapers?id=about
	priority is 0
	the current length of the visited set is : 432
crawled http://www.metroland.com/newspapers?id=about it was  news 
returning reqs
visiting http://www.vanguardngr.com/2015/01/terrorism-defence-hq-puts-deathtoll-baga-attack-150/
	priority is 2
	the current length of the visited set is : 433
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 6: ordinal not in range(128)

visiting http://www.toronto.com/
	priority is 2
	the current length of the visited set is : 434
crawled http://www.toronto.com/ it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/tag/national-geographic-traveler-photo-contest/
	priority is 2
	the current length of the visited set is : 436
crawled http://press.nationalgeographic.com/tag/national-geographic-traveler-photo-contest/ it was  news 
returning reqs
visiting http://www.leasebusters.com/en/default.asp
	priority is 2
	the current length of the visited set is : 437
crawled http://www.leasebusters.com/en/default.asp it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/tag/wayne-lawrence/
	priority is 2
	the current length of the visited set is : 438
crawled http://press.nationalgeographic.com/tag/wayne-lawrence/ it was  news 
returning reqs
visiting http://www.travelalerts.ca/
	priority is 2
	the current length of the visited set is : 439
crawled http://www.travelalerts.ca/ it wasn't  news 
returning reqs
visiting http://edition.cnn.com/help
	priority is 2
	the current length of the visited set is : 440
crawled http://edition.cnn.com/help it wasn't  news 
returning reqs
visiting http://www.punditsguide.ca/
	priority is 2
	the current length of the visited set is : 441
crawled http://www.punditsguide.ca/ it wasn't  news 
returning reqs
visiting http://www.electionresources.org/
	priority is 2
	the current length of the visited set is : 442
crawled http://www.electionresources.org/ it wasn't  news 
returning reqs
visiting http://www.electionalmanac.com/ea/canada-election-polls-atlantic-canada/
	priority is 2
	the current length of the visited set is : 443
crawled http://www.electionalmanac.com/ea/canada-election-polls-atlantic-canada/ it was  news 
returning reqs
visiting http://www.sfu.ca/~aheard/elections/index.htm
	priority is 2
	the current length of the visited set is : 444
crawled http://www.sfu.ca/~aheard/elections/index.htm it wasn't  news 
returning reqs
visiting http://www.metroland.com/working-at-metroland/
	priority is 0
	the current length of the visited set is : 445
crawled http://www.metroland.com/working-at-metroland/ it wasn't  news 
returning reqs
visiting http://www.lispop.ca/
	priority is 2
	the current length of the visited set is : 446
crawled http://www.lispop.ca/ it wasn't  news 
returning reqs
visiting https://twitter.com/paulrich_nb/status/390856668973502464/photo/1
	priority is 0
	the current length of the visited set is : 447
crawled https://twitter.com/paulrich_nb/status/390856668973502464/photo/1 it wasn't  news 
returning reqs
visiting http://www.electionprediction.org/
	priority is 2
	the current length of the visited set is : 448
crawled http://www.electionprediction.org/ it wasn't  news 
returning reqs
visiting http://save.ca/
	priority is 2
	the current length of the visited set is : 449
crawled http://save.ca/ it wasn't  news 
returning reqs
visiting http://esm.ubc.ca/forecast.php
	priority is 2
	the current length of the visited set is : 450
crawled http://esm.ubc.ca/forecast.php it wasn't  news 
returning reqs
visiting http://www.electionalmanac.com/ea/canada-election-polls-quebec/
	priority is 2
	the current length of the visited set is : 451
crawled http://www.electionalmanac.com/ea/canada-election-polls-quebec/ it was  news 
returning reqs
visiting http://www.hilltimes.com/
	priority is 2
	the current length of the visited set is : 452
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting http://press.nationalgeographic.com/tag/national-geographic-traveler/
	priority is 2
	the current length of the visited set is : 453
crawled http://press.nationalgeographic.com/tag/national-geographic-traveler/ it was  news 
returning reqs
visiting http://hockeynow.ca/
	priority is 2
	the current length of the visited set is : 454
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 131, in _process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 80, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)

visiting http://jobs.localwork.ca/jobs/rl-mississauga-ontario-canada/
	priority is 2
	the current length of the visited set is : 455
crawled http://jobs.localwork.ca/jobs/rl-mississauga-ontario-canada/ it wasn't  news 
returning reqs
visiting http://www.vice.com/en_us
	priority is 0
	the current length of the visited set is : 0
crawled http://www.vice.com/en_us it wasn't  news 
returning reqs
visiting http://www.vice.com/en_us
	priority is 0
	the current length of the visited set is : 0
visiting http://www.vice.com/en_us
	priority is 0
	the current length of the visited set is : 0
crawled http://www.vice.com/en_us it wasn't  news 
returning reqs
crawled http://www.vice.com/en_us it wasn't  news 
returning reqs
visiting http://www.vice.com/en_us
	priority is 0
	the current length of the visited set is : 0
crawled http://www.vice.com/en_us it wasn't  news 
returning reqs
visiting http://www.vice.com/en_us
	priority is 0
	the current length of the visited set is : 0
crawled http://www.vice.com/en_us it wasn't  news 
returning reqs
visiting http://www.vice.com/en_us
	priority is 0
	the current length of the visited set is : 0
crawled http://www.vice.com/en_us it wasn't  news 
returning reqs
visiting http://www.vice.com/en_us
	priority is 0
	the current length of the visited set is : 0
crawled http://www.vice.com/en_us it wasn't  news 
returning reqs
visiting http://noisey.vice.com/blog/getting-rid-of-your-vinyl-forever-heres-a-cookie
	priority is 0
	the current length of the visited set is : 1
crawled http://noisey.vice.com/blog/getting-rid-of-your-vinyl-forever-heres-a-cookie it wasn't  news 
returning reqs
visiting http://motherboard.vice.com/read/isis-vs-3d-printing
	priority is 0
	the current length of the visited set is : 2
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 15: ordinal not in range(128)

visiting http://munchies.vice.com/articles/why-is-brooklyn-barbecue-taking-over-the-world
	priority is 0
	the current length of the visited set is : 3
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 54: ordinal not in range(128)

visiting https://news.vice.com/article/islamic-state-warns-of-black-days-for-saudi-arabia-as-bomber-in-mosque-attack-is-identified
	priority is 0
	the current length of the visited set is : 4
crawled https://news.vice.com/article/islamic-state-warns-of-black-days-for-saudi-arabia-as-bomber-in-mosque-attack-is-identified it wasn't  news 
returning reqs
visiting http://noisey.vice.com/terms-of-use
	priority is 0
	the current length of the visited set is : 5
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)

visiting http://link.vice.com/join/3nh/viceussignup
	priority is 0
	the current length of the visited set is : 6
crawled http://link.vice.com/join/3nh/viceussignup it wasn't  news 
returning reqs
visiting http://store.vice.com/
	priority is 0
	the current length of the visited set is : 7
crawled http://store.vice.com/ it wasn't  news 
returning reqs
visiting http://motherboard.vice.com/terms
	priority is 0
	the current length of the visited set is : 8
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 7: ordinal not in range(128)

visiting http://www.vice.com/read/artists-of-today-part-2
	priority is 0
	the current length of the visited set is : 9
crawled http://www.vice.com/read/artists-of-today-part-2 it wasn't  news 
returning reqs
visiting http://noisey.vice.com/privacy-policy
	priority is 0
	the current length of the visited set is : 10
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting http://www.vice.com/articles
	priority is 0
	the current length of the visited set is : 11
crawled http://www.vice.com/articles it wasn't  news 
returning reqs
visiting http://www.asheavenue.com/
	priority is 0
	the current length of the visited set is : 12
crawled http://www.asheavenue.com/ it wasn't  news 
returning reqs
visiting http://www.metalsucks.net/2015/05/18/why-ive-stopped-buying-vinyl/
	priority is 0
	the current length of the visited set is : 13
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 11: ordinal not in range(128)

visiting http://store.vice.com/collections
	priority is 0
	the current length of the visited set is : 14
crawled http://store.vice.com/collections it wasn't  news 
returning reqs
visiting http://www.vice.com/author/batya-ungarsargon
	priority is 0
	the current length of the visited set is : 15
crawled http://www.vice.com/author/batya-ungarsargon it wasn't  news 
returning reqs
visiting https://twitter.com/vice
	priority is 0
	the current length of the visited set is : 16
crawled https://twitter.com/vice it wasn't  news 
returning reqs
visiting http://www.vice.com/comics
	priority is 0
	the current length of the visited set is : 17
crawled http://www.vice.com/comics it wasn't  news 
returning reqs
visiting http://noisey.vice.com/pages/jobs
	priority is 0
	the current length of the visited set is : 18
crawled http://noisey.vice.com/pages/jobs it wasn't  news 
returning reqs
visiting http://noisey.vice.com/about
	priority is 0
	the current length of the visited set is : 19
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 178: ordinal not in range(128)

visiting http://www.vice.com/read/a-tech-guy-from-detroit-created-a-dating-app-that-matches-israelis-with-palestinians
	priority is 0
	the current length of the visited set is : 20
crawled http://www.vice.com/read/a-tech-guy-from-detroit-created-a-dating-app-that-matches-israelis-with-palestinians it wasn't  news 
returning reqs
visiting http://www.vice.com/author/aleksandar-hemon
	priority is 0
	the current length of the visited set is : 21
crawled http://www.vice.com/author/aleksandar-hemon it wasn't  news 
returning reqs
visiting https://news.vice.com/article/11th-hour-deal-with-far-right-party-brings-netanyahu-back-for-fourth-term-as-israels-pm
	priority is 0
	the current length of the visited set is : 22
crawled https://news.vice.com/article/11th-hour-deal-with-far-right-party-brings-netanyahu-back-for-fourth-term-as-israels-pm it was  news 
returning reqs
visiting http://munchies.vice.com/tag/meat
	priority is 0
	the current length of the visited set is : 23
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting http://store.vice.com/products/subscribe-to-vice-magazine-for-the-ipad
	priority is 0
	the current length of the visited set is : 24
crawled http://store.vice.com/products/subscribe-to-vice-magazine-for-the-ipad it wasn't  news 
returning reqs
visiting http://www.shopify.com/
	priority is 0
	the current length of the visited set is : 25
crawled http://www.shopify.com/ it wasn't  news 
returning reqs
visiting http://www.vice.com/read/the-us-military-euthanized-or-abandoned-thousands-of-their-own-canine-soldiers-at-the-end-of-the-vietnam-war-253
	priority is 0
	the current length of the visited set is : 26
crawled http://www.vice.com/read/the-us-military-euthanized-or-abandoned-thousands-of-their-own-canine-soldiers-at-the-end-of-the-vietnam-war-253 it was  news 
returning reqs
visiting https://www.facebook.com/VICE
	priority is 0
	the current length of the visited set is : 27
crawled https://www.facebook.com/VICE it wasn't  news 
returning reqs
visiting http://noisey.vice.com/music-video-premieres/noonie-bao-im-in-love
	priority is 0
	the current length of the visited set is : 28
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)

visiting http://www.vice.com/read/the-inception-of-zombie-wars-0000654-v22n5
	priority is 0
	the current length of the visited set is : 29
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)

visiting http://noisey.vice.com/under-the-influence/under-the-influence-new-york-hardcore
	priority is 0
	the current length of the visited set is : 30
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6: ordinal not in range(128)

visiting https://news.vice.com/article/10-year-old-girl-raped-by-stepfather-is-refused-an-abortion-in-paraguay
	priority is 0
	the current length of the visited set is : 31
crawled https://news.vice.com/article/10-year-old-girl-raped-by-stepfather-is-refused-an-abortion-in-paraguay it wasn't  news 
returning reqs
visiting http://noisey.vice.com/there-will-be-quiet/there-will-be-quiet-the-story-of-judge-part-4
	priority is 0
	the current length of the visited set is : 32
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6: ordinal not in range(128)

visiting https://plus.google.com/+VICE
	priority is 0
	the current length of the visited set is : 33
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe8 in position 0: ordinal not in range(128)

visiting http://www.vice.com/author/charlie-ambler
	priority is 0
	the current length of the visited set is : 34
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)

visiting https://www.youtube.com/user/vice
	priority is 0
	the current length of the visited set is : 35
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 17: ordinal not in range(128)

visiting https://twitter.com/josiahmhesse
	priority is 2
	the current length of the visited set is : 36
crawled https://twitter.com/josiahmhesse it wasn't  news 
returning reqs
visiting http://www.vice.com/stuff
	priority is 0
	the current length of the visited set is : 37
crawled http://www.vice.com/stuff it wasn't  news 
returning reqs
visiting http://noisey.vice.com/blog/holly-herndons-profile-interview-2015
	priority is 0
	the current length of the visited set is : 38
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 34: ordinal not in range(128)

visiting http://www.vice.com/read/vice-exclusive-check-out-unsung-80s-post-punk-heroes-the-mothmen-with-their-new-reissue-815
	priority is 0
	the current length of the visited set is : 39
crawled http://www.vice.com/read/vice-exclusive-check-out-unsung-80s-post-punk-heroes-the-mothmen-with-their-new-reissue-815 it wasn't  news 
returning reqs
visiting http://noisey.vice.com/blog/skrillex-interview-2015
	priority is 0
	the current length of the visited set is : 40
crawled http://noisey.vice.com/blog/skrillex-interview-2015 it was  news 
returning reqs
visiting http://noisey.vice.com/blog/paul-mccartney-whistled-the-melody-to-kanyes-all-day-in-a-parkinson-interview-in-1999
	priority is 0
	the current length of the visited set is : 41
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0x80 in position 0: ordinal not in range(128)

visiting http://forefathersgroup.com/
	priority is 0
	the current length of the visited set is : 42
crawled http://forefathersgroup.com/ it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/al-bayan-radio
	priority is 0
	the current length of the visited set is : 43
crawled https://news.vice.com/topic/al-bayan-radio it wasn't  news 
returning reqs
visiting https://www.tumblr.com/register/follow/vicemag
	priority is 0
	the current length of the visited set is : 44
crawled https://www.tumblr.com/register/follow/vicemag it wasn't  news 
returning reqs
visiting http://www.vice.com/video/vice-meets-anas-aremeyaw-anas
	priority is 0
	the current length of the visited set is : 45
crawled http://www.vice.com/video/vice-meets-anas-aremeyaw-anas it wasn't  news 
returning reqs
visiting https://news.vice.com/article/42-dead-suspects-1-dead-agent-lopsided-shootout-in-mexico-is-under-suspicion
	priority is 2
	the current length of the visited set is : 46
crawled https://news.vice.com/article/42-dead-suspects-1-dead-agent-lopsided-shootout-in-mexico-is-under-suspicion it wasn't  news 
returning reqs
visiting http://noisey.vice.com/en_us/newsletter/subscribe
	priority is 0
	the current length of the visited set is : 47
crawled http://noisey.vice.com/en_us/newsletter/subscribe it wasn't  news 
returning reqs
visiting http://www.vice.com/film
	priority is 0
	the current length of the visited set is : 48
crawled http://www.vice.com/film it wasn't  news 
returning reqs
visiting http://www.vice.com/read/canadas-truth-commission-on-residential-schools-is-coming-to-a-troubling-close-far-from-reconciliation
	priority is 0
	the current length of the visited set is : 49
crawled http://www.vice.com/read/canadas-truth-commission-on-residential-schools-is-coming-to-a-troubling-close-far-from-reconciliation it was  news 
returning reqs
visiting https://news.vice.com/topic/bibi
	priority is 2
	the current length of the visited set is : 50
crawled https://news.vice.com/topic/bibi it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140519-dogs-war-canines-soldiers-troops-military-vietnam/
	priority is 2
	the current length of the visited set is : 51
crawled http://news.nationalgeographic.com/news/2014/05/140519-dogs-war-canines-soldiers-troops-military-vietnam/ it was  news 
returning reqs
visiting http://www.denverpost.com/news/ci_6003386
	priority is 2
	the current length of the visited set is : 52
crawled http://www.denverpost.com/news/ci_6003386 it wasn't  news 
returning reqs
visiting http://noisey.vice.com/stuff-we-love
	priority is 0
	the current length of the visited set is : 53
crawled http://noisey.vice.com/stuff-we-love it wasn't  news 
returning reqs
visiting http://www.cnn.com/2010/LIVING/02/12/war.dogs/
	priority is 2
	the current length of the visited set is : 54
crawled http://www.cnn.com/2010/LIVING/02/12/war.dogs/ it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/yisrael-beiteinu-party
	priority is 2
	the current length of the visited set is : 55
crawled https://news.vice.com/topic/yisrael-beiteinu-party it wasn't  news 
returning reqs
visiting http://www.vice.com/read/the-sand-looters-0000647-v22n5
	priority is 0
	the current length of the visited set is : 56
crawled http://www.vice.com/read/the-sand-looters-0000647-v22n5 it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/records
	priority is 0
	the current length of the visited set is : 57
crawled http://noisey.vice.com/tag/records it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/kulanu-party
	priority is 2
	the current length of the visited set is : 58
crawled https://news.vice.com/topic/kulanu-party it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/metal
	priority is 0
	the current length of the visited set is : 59
crawled http://noisey.vice.com/tag/metal it wasn't  news 
returning reqs
visiting http://www.vice.com/read/pc-andrew-ott-conviction-235
	priority is 0
	the current length of the visited set is : 60
crawled http://www.vice.com/read/pc-andrew-ott-conviction-235 it was  news 
returning reqs
visiting http://noisey.vice.com/tag/KRIEG
	priority is 0
	the current length of the visited set is : 61
crawled http://noisey.vice.com/tag/KRIEG it wasn't  news 
returning reqs
visiting https://twitter.com/NatGeo
	priority is 2
	the current length of the visited set is : 62
crawled https://twitter.com/NatGeo it wasn't  news 
returning reqs
visiting http://noisey.vice.com/blog/free-weed-introduction-video-premiere
	priority is 0
	the current length of the visited set is : 63
crawled http://noisey.vice.com/blog/free-weed-introduction-video-premiere it wasn't  news 
returning reqs
visiting http://www.vice.com/pages/privacy-and-terms
	priority is 0
	the current length of the visited set is : 64
crawled http://www.vice.com/pages/privacy-and-terms it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/jewish-home
	priority is 2
	the current length of the visited set is : 65
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 32: ordinal not in range(128)

visiting http://www.nationalgeographic.com/community/privacy/
	priority is 2
	the current length of the visited set is : 66
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting https://news.vice.com/topic/benjamin-netanyahu
	priority is 2
	the current length of the visited set is : 67
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 38: ordinal not in range(128)

visiting https://news.vice.com/topic/palestine
	priority is 2
	the current length of the visited set is : 68
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 37: ordinal not in range(128)

visiting http://noisey.vice.com/blog/dr-dre-kendrick-lamar-jeremih-2nite
	priority is 0
	the current length of the visited set is : 69
crawled http://noisey.vice.com/blog/dr-dre-kendrick-lamar-jeremih-2nite it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/Vinyl
	priority is 0
	the current length of the visited set is : 70
crawled http://noisey.vice.com/tag/Vinyl it wasn't  news 
returning reqs
visiting https://www.youtube.com/channel/UC0iwHRFpv2_fpojZgQhElEQ?sub_confirmation=1
	priority is 0
	the current length of the visited set is : 71
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 165: ordinal not in range(128)

visiting http://www.vice.com/pages/jobs
	priority is 0
	the current length of the visited set is : 72
crawled http://www.vice.com/pages/jobs it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/community/terms/
	priority is 2
	the current length of the visited set is : 73
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting http://www.vice.com/pages/about
	priority is 0
	the current length of the visited set is : 74
crawled http://www.vice.com/pages/about it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/community/email/
	priority is 2
	the current length of the visited set is : 75
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting https://twitter.com/WailQ
	priority is 2
	the current length of the visited set is : 76
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting http://noisey.vice.com/blog/starlito-introversion-mixtape-premiere
	priority is 0
	the current length of the visited set is : 77
crawled http://noisey.vice.com/blog/starlito-introversion-mixtape-premiere it wasn't  news 
returning reqs
visiting https://www.facebook.com/natgeo
	priority is 2
	the current length of the visited set is : 78
crawled https://www.facebook.com/natgeo it wasn't  news 
returning reqs
visiting http://noisey.vice.com/blog/sean-leon-wondagurl-river-tiber-this-aint-2012
	priority is 0
	the current length of the visited set is : 79
crawled http://noisey.vice.com/blog/sean-leon-wondagurl-river-tiber-this-aint-2012 it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/siteindex/customer/
	priority is 2
	the current length of the visited set is : 80
crawled http://www.nationalgeographic.com/siteindex/customer/ it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/magazines/
	priority is 2
	the current length of the visited set is : 81
crawled http://www.nationalgeographic.com/magazines/ it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/lgbt
	priority is 0
	the current length of the visited set is : 82
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting http://www.vice.com/rss
	priority is 0
	the current length of the visited set is : 83
crawled http://www.vice.com/rss it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/mediakit/
	priority is 2
	the current length of the visited set is : 84
crawled http://www.nationalgeographic.com/mediakit/ it wasn't  news 
returning reqs
visiting http://noisey.vice.com/en_uk/blog/how-independent-artists-and-labels-are-getting-squeezed-out-by-the-vinyl-revival
	priority is 0
	the current length of the visited set is : 85
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0x80 in position 0: ordinal not in range(128)

visiting http://www.vice.com/tag/opinion
	priority is 0
	the current length of the visited set is : 86
crawled http://www.vice.com/tag/opinion it wasn't  news 
returning reqs
visiting http://events.nationalgeographic.com/
	priority is 2
	the current length of the visited set is : 87
crawled http://events.nationalgeographic.com/ it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/technology
	priority is 2
	the current length of the visited set is : 88
crawled http://noisey.vice.com/tag/technology it wasn't  news 
returning reqs
visiting http://press.nationalgeographic.com/
	priority is 2
	the current length of the visited set is : 89
crawled http://press.nationalgeographic.com/ it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/siteindex/international/
	priority is 2
	the current length of the visited set is : 90
crawled http://www.nationalgeographic.com/siteindex/international/ it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/culture
	priority is 0
	the current length of the visited set is : 91
crawled http://www.vice.com/tag/culture it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/Aphex+Twin
	priority is 2
	the current length of the visited set is : 92
crawled http://noisey.vice.com/tag/Aphex+Twin it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/riddim
	priority is 2
	the current length of the visited set is : 93
crawled http://noisey.vice.com/tag/riddim it wasn't  news 
returning reqs
visiting http://shop.nationalgeographic.com/ngs/index.jsp
	priority is 2
	the current length of the visited set is : 94
crawled http://shop.nationalgeographic.com/ngs/index.jsp it wasn't  news 
returning reqs
visiting http://www.vice.com/series/the-vice-guide-to-mental-health
	priority is 0
	the current length of the visited set is : 95
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6: ordinal not in range(128)

visiting http://events.nationalgeographic.com/exhibits/
	priority is 0
	the current length of the visited set is : 96
crawled http://events.nationalgeographic.com/exhibits/ it wasn't  news 
returning reqs
visiting https://instagram.com/natgeo/
	priority is 0
	the current length of the visited set is : 97
crawled https://instagram.com/natgeo/ it wasn't  news 
returning reqs
visiting http://www.defendtherighttoprotest.org/police-on-trial-for-2010-student-fees-protests-ipcc-release-audio-recording/
	priority is 2
	the current length of the visited set is : 98
crawled http://www.defendtherighttoprotest.org/police-on-trial-for-2010-student-fees-protests-ipcc-release-audio-recording/ it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/dubstep
	priority is 2
	the current length of the visited set is : 99
crawled http://noisey.vice.com/tag/dubstep it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/politics
	priority is 0
	the current length of the visited set is : 100
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting https://news.vice.com/topic/israel
	priority is 2
	the current length of the visited set is : 101
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 32: ordinal not in range(128)

visiting http://www.vice.com/tag/crime
	priority is 0
	the current length of the visited set is : 102
crawled http://www.vice.com/tag/crime it wasn't  news 
returning reqs
visiting http://www.nationalgeographic.com/about/
	priority is 2
	the current length of the visited set is : 103
crawled http://www.nationalgeographic.com/about/ it wasn't  news 
returning reqs
visiting http://www.vice.com/magazine
	priority is 0
	the current length of the visited set is : 104
crawled http://www.vice.com/magazine it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/EDM
	priority is 2
	the current length of the visited set is : 105
crawled http://noisey.vice.com/tag/EDM it wasn't  news 
returning reqs
visiting https://donate.nationalgeographic.org/Page.aspx
	priority is 2
	the current length of the visited set is : 106
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 1: ordinal not in range(128)

visiting http://www.nationalgeographic.com/jobs/
	priority is 2
	the current length of the visited set is : 107
crawled http://www.nationalgeographic.com/jobs/ it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/Emojis
	priority is 2
	the current length of the visited set is : 108
crawled http://noisey.vice.com/tag/Emojis it wasn't  news 
returning reqs
visiting http://www.vice.com/photos
	priority is 0
	the current length of the visited set is : 109
crawled http://www.vice.com/photos it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/Mission+to+Mars
	priority is 2
	the current length of the visited set is : 110
crawled http://noisey.vice.com/tag/Mission+to+Mars it wasn't  news 
returning reqs
visiting http://www.vice.com/dnd
	priority is 0
	the current length of the visited set is : 111
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 243: ordinal not in range(128)

visiting http://news.nationalgeographic.com/2015/05/150514-indiana-jones-archaeology-exhibit-national-geographic-museum/
	priority is 0
	the current length of the visited set is : 112
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://noisey.vice.com/tag/satellites
	priority is 2
	the current length of the visited set is : 113
crawled http://noisey.vice.com/tag/satellites it wasn't  news 
returning reqs
visiting http://www.vice.com/nsfw
	priority is 0
	the current length of the visited set is : 114
crawled http://www.vice.com/nsfw it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140518-dogs-war-canines-soldiers-troops-military-japanese-prisoner/
	priority is 2
	the current length of the visited set is : 115
crawled http://news.nationalgeographic.com/news/2014/05/140518-dogs-war-canines-soldiers-troops-military-japanese-prisoner/ it was  news 
returning reqs
visiting http://www.vice.com/gaming
	priority is 0
	the current length of the visited set is : 116
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting http://noisey.vice.com/tag/Google+Editions
	priority is 2
	the current length of the visited set is : 117
crawled http://noisey.vice.com/tag/Google+Editions it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140517-dogs-war-canines-soldiers-troops-marine-military-pacific-japan/
	priority is 2
	the current length of the visited set is : 118
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)

visiting http://noisey.vice.com/tag/android
	priority is 2
	the current length of the visited set is : 119
crawled http://noisey.vice.com/tag/android it wasn't  news 
returning reqs
visiting http://www.amazon.com/War-Dogs-Canine-Heroism-History/dp/1137279680
	priority is 0
	the current length of the visited set is : 120
crawled http://www.amazon.com/War-Dogs-Canine-Heroism-History/dp/1137279680 it wasn't  news 
returning reqs
visiting http://www.vice.com/food
	priority is 0
	the current length of the visited set is : 121
crawled http://www.vice.com/food it wasn't  news 
returning reqs
visiting http://noisey.vice.com/tag/Google
	priority is 2
	the current length of the visited set is : 122
crawled http://noisey.vice.com/tag/Google it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140516-dogs-war-canines-soldiers-troops-army-military/
	priority is 2
	the current length of the visited set is : 123
crawled http://news.nationalgeographic.com/news/2014/05/140516-dogs-war-canines-soldiers-troops-army-military/ it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/
	priority is 2
	the current length of the visited set is : 124
crawled http://news.nationalgeographic.com/ it wasn't  news 
returning reqs
visiting https://twitter.com/mcslo
	priority is 2
	the current length of the visited set is : 125
crawled https://twitter.com/mcslo it wasn't  news 
returning reqs
visiting http://www.vice.com/tech
	priority is 0
	the current length of the visited set is : 126
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 3: ordinal not in range(128)

visiting http://www.vice.com/sports
	priority is 0
	the current length of the visited set is : 127
crawled http://www.vice.com/sports it wasn't  news 
returning reqs
visiting http://www.vice.com/travel
	priority is 0
	the current length of the visited set is : 128
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting http://www.vice.com/fashion
	priority is 0
	the current length of the visited set is : 129
crawled http://www.vice.com/fashion it wasn't  news 
returning reqs
visiting http://www.vice.com/music
	priority is 0
	the current length of the visited set is : 130
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)

visiting http://news.nationalgeographic.com/2015/05/150518-cahokia-ancient-America-prehistoric-floods-mystery-Mississippi/
	priority is 0
	the current length of the visited set is : 131
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://www.vice.com/news
	priority is 0
	the current length of the visited set is : 132
crawled http://www.vice.com/news it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/05/150516-syria-palmyra-islamic-state-ISIS-ISIL-ancient-Rome-destruction-world-heritage-archaeology/
	priority is 0
	the current length of the visited set is : 133
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 14: ordinal not in range(128)

visiting http://www.pdsa.org.uk/what-we-do/animal-honours/pdsa-dickin-medal
	priority is 0
	the current length of the visited set is : 134
crawled http://www.pdsa.org.uk/what-we-do/animal-honours/pdsa-dickin-medal it wasn't  news 
returning reqs
visiting http://www.canada.com/victoriatimescolonist/news/comment/story.html
	priority is 2
	the current length of the visited set is : 135
crawled http://www.canada.com/victoriatimescolonist/news/comment/story.html it wasn't  news 
returning reqs
visiting http://www.thestar.com/news/canada/2012/06/11/four_years_later_harpers_apology_for_residential_schools_rings_hollow_for_many.html
	priority is 2
	the current length of the visited set is : 136
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting http://www.vice.com/videos
	priority is 0
	the current length of the visited set is : 137
crawled http://www.vice.com/videos it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/canada/prime-minister-stephen-harper-s-statement-of-apology-1.734250
	priority is 2
	the current length of the visited set is : 138
crawled http://www.cbc.ca/news/canada/prime-minister-stephen-harper-s-statement-of-apology-1.734250 it wasn't  news 
returning reqs
visiting https://members.nationalgeographic.com/s/?next=/_membercenter/
	priority is 0
	the current length of the visited set is : 139
crawled https://members.nationalgeographic.com/s/?next=/_membercenter/ it wasn't  news 
returning reqs
visiting http://www.theglobeandmail.com/news/national/in-photos-st-michaels-residential-school-demolition-ceremony/article23067843/
	priority is 2
	the current length of the visited set is : 140
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)

visiting http://www.scielo.org.za/scielo.php
	priority is 2
	the current length of the visited set is : 141
crawled http://www.scielo.org.za/scielo.php it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/04/150418-abraham-lincoln-funeral-train-railroad-civil-war-history/
	priority is 0
	the current length of the visited set is : 142
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://reconciliationcanada.ca/welcome/who-we-are/
	priority is 2
	the current length of the visited set is : 143
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 323: ordinal not in range(128)

visiting http://www.vice.com/read/the-blobby-boys-meet-kiss-298
	priority is 0
	the current length of the visited set is : 144
crawled http://www.vice.com/read/the-blobby-boys-meet-kiss-298 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/canada/manitoba/canada-s-truth-commission-learned-from-mandela-says-head-1.2454851
	priority is 2
	the current length of the visited set is : 145
crawled http://www.cbc.ca/news/canada/manitoba/canada-s-truth-commission-learned-from-mandela-says-head-1.2454851 it wasn't  news 
returning reqs
visiting http://www.canada.com/ottawacitizen/story.html
	priority is 2
	the current length of the visited set is : 146
crawled http://www.canada.com/ottawacitizen/story.html it wasn't  news 
returning reqs
visiting http://www.thestar.com/news/canada/2010/07/23/no_truth_no_reconciliation_for_aging_residential_school_survivors.html
	priority is 2
	the current length of the visited set is : 147
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)

visiting http://www.cbc.ca/news/canada/manitoba/truth-is-hard-but-residential-school-reconciliation-harder-murray-sinclair-1.2819931
	priority is 2
	the current length of the visited set is : 148
crawled http://www.cbc.ca/news/canada/manitoba/truth-is-hard-but-residential-school-reconciliation-harder-murray-sinclair-1.2819931 it wasn't  news 
returning reqs
visiting http://www.vice.com/read/megg-mogg-owl-high-school-298
	priority is 0
	the current length of the visited set is : 149
crawled http://www.vice.com/read/megg-mogg-owl-high-school-298 it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/04/150415-ngbooktalk-nazis-auschwitz-holocaust-survivors/
	priority is 0
	the current length of the visited set is : 150
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting http://www.vice.com/read/michael-michael-fantasy
	priority is 0
	the current length of the visited set is : 151
crawled http://www.vice.com/read/michael-michael-fantasy it wasn't  news 
returning reqs
visiting http://aptn.ca/news/2013/04/30/trc-ready-to-again-take-residential-school-document-fight-with-ottawa-to-court/
	priority is 2
	the current length of the visited set is : 152
crawled http://aptn.ca/news/2013/04/30/trc-ready-to-again-take-residential-school-document-fight-with-ottawa-to-court/ it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/05/150508-wwii-planes-flyover-veterans-ve-day-victory-in-europe-video/
	priority is 0
	the current length of the visited set is : 153
crawled http://news.nationalgeographic.com/2015/05/150508-wwii-planes-flyover-veterans-ve-day-victory-in-europe-video/ it wasn't  news 
returning reqs
visiting https://www.flickr.com/photos/aadncanada/16731932098/in/photolist-ruxwtU-rM3c7M-ruyJ6S-qQkB6g-rM3bZH-rM1QYJ-9XpAoS-9XmHX4-9XpA39-9XpzUL-rJPvQG-5gCEqy-pJM6s-pJM6C-ruyHAy-qQkB3R-ruxvYL-qQkAzB-5gGtiT-q32UX-pJM6v-gub4Cu-omDSt7-qmqQ9E-r2121R-qmEWEM-r1UDLC-r1U35s-qZ6vGP-rirKC4-riqJdc-qmrCFN-r1VKpj-r1UEWm-rinQC9-r22K9g-qms4V5-r1SAB3-rijnCc-qmDbG8-r221aR-qmE9Qa-rik1RR-qmsr3f-r1TRbo-ripdZ1-rijgHB-r1Zdze-riqibZ-qmG4Pz
	priority is 2
	the current length of the visited set is : 154
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 20: ordinal not in range(128)

visiting http://www.cbc.ca/news/politics/ottawa-ordered-to-provide-all-residential-schools-documents-1.1345892
	priority is 2
	the current length of the visited set is : 155
crawled http://www.cbc.ca/news/politics/ottawa-ordered-to-provide-all-residential-schools-documents-1.1345892 it was  news 
returning reqs
visiting http://www.theglobeandmail.com/news/politics/at-reconciliation-commission-key-positions-remain-unfilled/article4313620/
	priority is 2
	the current length of the visited set is : 156
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 27: ordinal not in range(128)

visiting http://www.vice.com/read/blood-lady-commandos-orange-party-balls
	priority is 0
	the current length of the visited set is : 157
crawled http://www.vice.com/read/blood-lady-commandos-orange-party-balls it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/aboriginal/residential-school-survivors-event-draws-thousands-in-edmonton-1.2588752
	priority is 2
	the current length of the visited set is : 158
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 128: ordinal not in range(128)

visiting http://www.cbc.ca/news/canada/british-columbia/thousands-walk-for-reconciliation-in-vancouver-1.1864051
	priority is 2
	the current length of the visited set is : 159
crawled http://www.cbc.ca/news/canada/british-columbia/thousands-walk-for-reconciliation-in-vancouver-1.1864051 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/paul-martin-accuses-residential-schools-of-cultural-genocide-1.1335199
	priority is 2
	the current length of the visited set is : 160
crawled http://www.cbc.ca/news/politics/paul-martin-accuses-residential-schools-of-cultural-genocide-1.1335199 it wasn't  news 
returning reqs
visiting http://www.theguardian.com/environment/true-north/2015/mar/03/documents-harper-pushing-first-nations-to-shelve-rights-buy-into-resource-rush
	priority is 2
	the current length of the visited set is : 161
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://www.anglican.ca/relationships/apology/english
	priority is 2
	the current length of the visited set is : 162
crawled http://www.anglican.ca/relationships/apology/english it wasn't  news 
returning reqs
visiting http://www.straight.com/news/428331/residential-school-survivors-share-their-stories-truth-and-reconciliation-event-vancouver
	priority is 2
	the current length of the visited set is : 163
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)

visiting http://www.vice.com/read/alex-sturrock-street-photography-373
	priority is 0
	the current length of the visited set is : 164
crawled http://www.vice.com/read/alex-sturrock-street-photography-373 it wasn't  news 
returning reqs
visiting http://rabble.ca/blogs/bloggers/derrick/2009/09/harper-denial-g20-canada-has-no-history-colonialism
	priority is 2
	the current length of the visited set is : 165
crawled http://rabble.ca/blogs/bloggers/derrick/2009/09/harper-denial-g20-canada-has-no-history-colonialism it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news2/interactives/electoral-district-search/
	priority is 2
	the current length of the visited set is : 166
crawled http://www.cbc.ca/news2/interactives/electoral-district-search/ it wasn't  news 
returning reqs
visiting http://www.anglican.ca/relationships/histories/gordons-school-punnichy
	priority is 2
	the current length of the visited set is : 167
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 2: ordinal not in range(128)

visiting http://news.nationalgeographic.com/news/2014/05/140520-myanmar-kachin-independence-army-illegal-logging-teak-china-world/
	priority is 0
	the current length of the visited set is : 168
crawled http://news.nationalgeographic.com/news/2014/05/140520-myanmar-kachin-independence-army-illegal-logging-teak-china-world/ it was  news 
returning reqs
visiting http://www.cbc.ca/news/politics/tfsas-who-really-benefits-from-new-contribution-limits-1.3066478
	priority is 2
	the current length of the visited set is : 169
crawled http://www.cbc.ca/news/politics/tfsas-who-really-benefits-from-new-contribution-limits-1.3066478 it wasn't  news 
returning reqs
visiting http://www.vice.com/read/questions-wed-like-to-ask-the-masonic-fraternal-police-department-882
	priority is 0
	the current length of the visited set is : 170
crawled http://www.vice.com/read/questions-wed-like-to-ask-the-masonic-fraternal-police-department-882 it was  news 
returning reqs
visiting http://www.cccb.ca/site/eng/media-room/files/2630-apology-on-residential-schools-by-the-catholic-church
	priority is 2
	the current length of the visited set is : 171
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 9: ordinal not in range(128)

visiting http://www.anglican.ca/news/finding-hildas-grave/3003619/
	priority is 2
	the current length of the visited set is : 172
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting http://www.amazon.ca/Kill-Indian-Save-Man-Residential/dp/0872864340
	priority is 2
	the current length of the visited set is : 173
crawled http://www.amazon.ca/Kill-Indian-Save-Man-Residential/dp/0872864340 it wasn't  news 
returning reqs
visiting https://www.aadnc-aandc.gc.ca/eng/1332949137290/1332949312397
	priority is 2
	the current length of the visited set is : 174
crawled https://www.aadnc-aandc.gc.ca/eng/1332949137290/1332949312397 it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140519-kawasaki-disease-japan-china-wind-science/
	priority is 0
	the current length of the visited set is : 175
crawled http://news.nationalgeographic.com/news/2014/05/140519-kawasaki-disease-japan-china-wind-science/ it wasn't  news 
returning reqs
visiting https://www.aadnc-aandc.gc.ca/eng/1332939430258/1332939552554
	priority is 2
	the current length of the visited set is : 176
crawled https://www.aadnc-aandc.gc.ca/eng/1332939430258/1332939552554 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/blue-and-gold-business-cards-for-u-k-staff-broke-rules-ottawa-admits-1.3061764
	priority is 2
	the current length of the visited set is : 177
crawled http://www.cbc.ca/news/politics/blue-and-gold-business-cards-for-u-k-staff-broke-rules-ottawa-admits-1.3061764 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.ca/2013/10/18/genocide-first-nations-aboriginals-canada-un_n_4123112.html
	priority is 2
	the current length of the visited set is : 178
crawled http://www.huffingtonpost.ca/2013/10/18/genocide-first-nations-aboriginals-canada-un_n_4123112.html it was  news 
returning reqs
visiting http://www.vice.com/comics/page/36
	priority is 0
	the current length of the visited set is : 179
crawled http://www.vice.com/comics/page/36 it wasn't  news 
returning reqs
visiting http://www.vice.com/comics/page/5
	priority is 0
	the current length of the visited set is : 180
crawled http://www.vice.com/comics/page/5 it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140516-wolves-oregon-or7-science-endangered-species-animals/
	priority is 0
	the current length of the visited set is : 181
crawled http://news.nationalgeographic.com/news/2014/05/140516-wolves-oregon-or7-science-endangered-species-animals/ it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/raising-kids-getting-more-affordable-government-analysis-suggests-1.3070037
	priority is 2
	the current length of the visited set is : 182
crawled http://www.cbc.ca/news/politics/raising-kids-getting-more-affordable-government-analysis-suggests-1.3070037 it was  news 
returning reqs
visiting http://www.vice.com/comics/page/4
	priority is 0
	the current length of the visited set is : 183
crawled http://www.vice.com/comics/page/4 it wasn't  news 
returning reqs
visiting http://www.trc.ca/websites/trcinstitution/index.php?p=26
	priority is 0
	the current length of the visited set is : 184
crawled http://www.trc.ca/websites/trcinstitution/index.php?p=26 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/anti-abortion-campaign-takes-on-assisted-suicide-ruling-in-election-year-1.3072330
	priority is 2
	the current length of the visited set is : 185
crawled http://www.cbc.ca/news/politics/anti-abortion-campaign-takes-on-assisted-suicide-ruling-in-election-year-1.3072330 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/hootsuite-founder-c-51-critic-ryan-holmes-tweets-ndp-thanks-1.3073890
	priority is 2
	the current length of the visited set is : 186
crawled http://www.cbc.ca/news/politics/hootsuite-founder-c-51-critic-ryan-holmes-tweets-ndp-thanks-1.3073890 it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/Tinder
	priority is 0
	the current length of the visited set is : 187
crawled http://www.vice.com/tag/Tinder it was  news 
returning reqs
visiting https://en.wikipedia.org/wiki/Genocide_Convention
	priority is 2
	the current length of the visited set is : 188
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xd7 in position 0: ordinal not in range(128)

visiting https://books.google.ca/books
	priority is 2
	the current length of the visited set is : 190
crawled https://books.google.ca/books it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/giving-tree-foundation-to-be-stripped-of-charity-status-1.3074967
	priority is 2
	the current length of the visited set is : 191
crawled http://www.cbc.ca/news/politics/giving-tree-foundation-to-be-stripped-of-charity-status-1.3074967 it was  news 
returning reqs
visiting http://www.vice.com/tag/love
	priority is 0
	the current length of the visited set is : 192
crawled http://www.vice.com/tag/love it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/black-voices/
	priority is 2
	the current length of the visited set is : 193
crawled http://www.huffingtonpost.com/black-voices/ it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/senators-call-for-new-blood-on-committee-dealing-with-audit-fallout-1.3074904
	priority is 2
	the current length of the visited set is : 194
crawled http://www.cbc.ca/news/politics/senators-call-for-new-blood-on-committee-dealing-with-audit-fallout-1.3074904 it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/romance
	priority is 0
	the current length of the visited set is : 195
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 27: ordinal not in range(128)

visiting http://www.huffingtonpost.com/books/
	priority is 2
	the current length of the visited set is : 196
crawled http://www.huffingtonpost.com/books/ it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/federal-election-2015-how-media-coverage-of-debates-could-change-1.3076190
	priority is 2
	the current length of the visited set is : 197
crawled http://www.cbc.ca/news/politics/federal-election-2015-how-media-coverage-of-debates-could-change-1.3076190 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/2015/05/25/pothole-mosaics_n_7407438.html
	priority is 2
	the current length of the visited set is : 198
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6: ordinal not in range(128)

visiting http://www.cbc.ca/news/politics/pierre-poilievre-rejects-criticism-over-taxpayer-funded-vanity-videos-1.3076240
	priority is 2
	the current length of the visited set is : 199
crawled http://www.cbc.ca/news/politics/pierre-poilievre-rejects-criticism-over-taxpayer-funded-vanity-videos-1.3076240 it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/dating
	priority is 0
	the current length of the visited set is : 200
crawled http://www.vice.com/tag/dating it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/canada/prince-edward-island/liberal-alan-mcisaac-wins-seat-in-coin-toss-after-recount-tie-1.3079433
	priority is 2
	the current length of the visited set is : 201
crawled http://www.cbc.ca/news/canada/prince-edward-island/liberal-alan-mcisaac-wins-seat-in-coin-toss-after-recount-tie-1.3079433 it wasn't  news 
returning reqs
visiting http://www.huffingtonpost.com/arts/
	priority is 2
	the current length of the visited set is : 202
crawled http://www.huffingtonpost.com/arts/ it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/online+dating
	priority is 0
	the current length of the visited set is : 203
crawled http://www.vice.com/tag/online+dating it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/war
	priority is 2
	the current length of the visited set is : 204
crawled http://www.vice.com/tag/war it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/jean-chretien-invited-putin-to-send-emissary-to-ex-leaders-meeting-next-month-1.3079374
	priority is 2
	the current length of the visited set is : 205
crawled http://www.cbc.ca/news/politics/jean-chretien-invited-putin-to-send-emissary-to-ex-leaders-meeting-next-month-1.3079374 it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/veteran
	priority is 2
	the current length of the visited set is : 206
crawled http://www.vice.com/tag/veteran it wasn't  news 
returning reqs
visiting http://unworthydominanceofregina.uregina.wikispaces.net/8+-+The+Story+of+Canada's+Indian+Residential+School+System?responseToken=4c6ca6d453335b9e1fcecba63f67a3d9
	priority is 0
	the current length of the visited set is : 207
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://www.cbc.ca/news/politics/ndp-jumps-into-3-way-race-with-conservatives-liberals-1.3079391
	priority is 2
	the current length of the visited set is : 208
crawled http://www.cbc.ca/news/politics/ndp-jumps-into-3-way-race-with-conservatives-liberals-1.3079391 it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/military
	priority is 2
	the current length of the visited set is : 209
crawled http://www.vice.com/tag/military it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/the-ruler-and-the-hero-do-the-federal-leaders-follow-common-types-1.3081295
	priority is 2
	the current length of the visited set is : 210
crawled http://www.cbc.ca/news/politics/the-ruler-and-the-hero-do-the-federal-leaders-follow-common-types-1.3081295 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/canada-can-t-account-for-3-1b-in-anti-terror-funding-ag-finds-1.1303999
	priority is 2
	the current length of the visited set is : 211
crawled http://www.cbc.ca/news/politics/canada-can-t-account-for-3-1b-in-anti-terror-funding-ag-finds-1.1303999 it was  news 
returning reqs
visiting http://www.vice.com/tag/dogs
	priority is 2
	the current length of the visited set is : 212
crawled http://www.vice.com/tag/dogs it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/Vietnam
	priority is 2
	the current length of the visited set is : 213
crawled http://www.vice.com/tag/Vietnam it was  news 
returning reqs
visiting http://www.cbc.ca/news/politics/20m-sex-work-exit-fund-over-subscribed-peter-mackay-1.3086064
	priority is 2
	the current length of the visited set is : 214
crawled http://www.cbc.ca/news/politics/20m-sex-work-exit-fund-over-subscribed-peter-mackay-1.3086064 it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/Vietnam+War
	priority is 2
	the current length of the visited set is : 215
crawled http://www.vice.com/tag/Vietnam+War it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/parties-playing-game-of-chicken-with-federal-election-debates-1.3084522
	priority is 2
	the current length of the visited set is : 216
crawled http://www.cbc.ca/news/politics/parties-playing-game-of-chicken-with-federal-election-debates-1.3084522 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/ruffled-feathers-power-plays-canada-s-first-tv-election-debate-was-also-a-headache-1.3086026
	priority is 2
	the current length of the visited set is : 217
crawled http://www.cbc.ca/news/politics/ruffled-feathers-power-plays-canada-s-first-tv-election-debate-was-also-a-headache-1.3086026 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/quick-facts-about-the-ndp-s-15-a-day-child-care-plan-1.2798328
	priority is 2
	the current length of the visited set is : 218
crawled http://www.cbc.ca/news/politics/quick-facts-about-the-ndp-s-15-a-day-child-care-plan-1.2798328 it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/Will+Horner
	priority is 2
	the current length of the visited set is : 219
crawled http://www.vice.com/tag/Will+Horner it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/Views+My+Own
	priority is 2
	the current length of the visited set is : 220
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 18: ordinal not in range(128)

visiting https://twitter.com/DeanBeeby
	priority is 0
	the current length of the visited set is : 221
crawled https://twitter.com/DeanBeeby it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/business/child-care-affordability-varies-widely-across-canada-1.2829817
	priority is 2
	the current length of the visited set is : 222
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://www.vice.com/tag/ABH
	priority is 2
	the current length of the visited set is : 223
crawled http://www.vice.com/tag/ABH it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/PC+Andrew+Ott
	priority is 2
	the current length of the visited set is : 224
crawled http://www.vice.com/tag/PC+Andrew+Ott it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/ndp-says-quality-affordable-child-care-just-an-election-away-1.2797570
	priority is 2
	the current length of the visited set is : 225
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)

visiting http://www.vice.com/tag/Student+protest
	priority is 2
	the current length of the visited set is : 226
crawled http://www.vice.com/tag/Student+protest it wasn't  news 
returning reqs
visiting https://www.vice.com/tag/vietnam%20war
	priority is 2
	the current length of the visited set is : 227
crawled https://www.vice.com/tag/vietnam%20war it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/justin-trudeau-hints-at-national-child-care-plan-tied-to-income-1.3065235
	priority is 2
	the current length of the visited set is : 228
crawled http://www.cbc.ca/news/politics/justin-trudeau-hints-at-national-child-care-plan-tied-to-income-1.3065235 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/harper-reincarnates-family-allowance-with-universal-child-care-benefit-1.3018557
	priority is 2
	the current length of the visited set is : 229
crawled http://www.cbc.ca/news/politics/harper-reincarnates-family-allowance-with-universal-child-care-benefit-1.3018557 it wasn't  news 
returning reqs
visiting http://www.vice.com/read/a-dene-alliance-formed-to-resist-uranium-and-tar-sands-mining-in-saskatchewan-892
	priority is 2
	the current length of the visited set is : 230
crawled http://www.vice.com/read/a-dene-alliance-formed-to-resist-uranium-and-tar-sands-mining-in-saskatchewan-892 it was  news 
returning reqs
visiting http://motherboard.vice.com/blog/new-evidence-that-the-chemical-curse-of-agent-orange-spread-to-peacetime
	priority is 2
	the current length of the visited set is : 231
crawled http://motherboard.vice.com/blog/new-evidence-that-the-chemical-curse-of-agent-orange-spread-to-peacetime it wasn't  news 
returning reqs
visiting http://www.vice.com/read/why-the-sister-of-a-murdered-aboriginal-woman-is-opposing-a-national-inquiry-952
	priority is 2
	the current length of the visited set is : 232
crawled http://www.vice.com/read/why-the-sister-of-a-murdered-aboriginal-woman-is-opposing-a-national-inquiry-952 it was  news 
returning reqs
visiting http://www.cbc.ca/news/canada/charitable-giving-how-emotion-peer-pressure-influence-donations-1.2451910
	priority is 2
	the current length of the visited set is : 233
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting http://www.vice.com/read/two-kids-died-in-a-house-fire-on-a-reserve-as-firefighters-stayed-put-over-a-3300-unpaid-bill-934
	priority is 2
	the current length of the visited set is : 234
crawled http://www.vice.com/read/two-kids-died-in-a-house-fire-on-a-reserve-as-firefighters-stayed-put-over-a-3300-unpaid-bill-934 it was  news 
returning reqs
visiting http://www.vice.com/read/the-canadian-government-is-withholding-documents-concerning-the-torture-of-native-children
	priority is 2
	the current length of the visited set is : 235
crawled http://www.vice.com/read/the-canadian-government-is-withholding-documents-concerning-the-torture-of-native-children it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/charities-seek-clarity-on-contributing-to-public-policy-debates-1.2981843
	priority is 2
	the current length of the visited set is : 236
crawled http://www.cbc.ca/news/politics/charities-seek-clarity-on-contributing-to-public-policy-debates-1.2981843 it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/revenue-canada-targets-steelworkers-charity-for-political-activities-1.3026863
	priority is 2
	the current length of the visited set is : 237
crawled http://www.cbc.ca/news/politics/revenue-canada-targets-steelworkers-charity-for-political-activities-1.3026863 it was  news 
returning reqs
visiting http://www.vice.com/tag/VICE+Canada
	priority is 2
	the current length of the visited set is : 238
crawled http://www.vice.com/tag/VICE+Canada it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/politics/sierra-club-latest-environmental-charity-hit-by-revenue-canada-audits-1.3052770
	priority is 2
	the current length of the visited set is : 239
crawled http://www.cbc.ca/news/politics/sierra-club-latest-environmental-charity-hit-by-revenue-canada-audits-1.3052770 it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/Murray+Sinclair
	priority is 2
	the current length of the visited set is : 240
crawled http://www.vice.com/tag/Murray+Sinclair it wasn't  news 
returning reqs
visiting http://www.vice.com/en_us
	priority is 0
	the current length of the visited set is : 0
crawled http://www.vice.com/en_us it wasn't  news 
returning reqs
visiting http://www.vice.com/articles
	priority is 0
	the current length of the visited set is : 1
crawled http://www.vice.com/articles it wasn't  news 
returning reqs
visiting http://noisey.vice.com/blog/getting-rid-of-your-vinyl-forever-heres-a-cookie
	priority is 0
	the current length of the visited set is : 2
crawled http://noisey.vice.com/blog/getting-rid-of-your-vinyl-forever-heres-a-cookie it wasn't  news 
returning reqs
visiting http://link.vice.com/join/3nh/viceussignup
	priority is 0
	the current length of the visited set is : 3
crawled http://link.vice.com/join/3nh/viceussignup it wasn't  news 
returning reqs
visiting https://news.vice.com/article/islamic-state-warns-of-black-days-for-saudi-arabia-as-bomber-in-mosque-attack-is-identified
	priority is 0
	the current length of the visited set is : 4
crawled https://news.vice.com/article/islamic-state-warns-of-black-days-for-saudi-arabia-as-bomber-in-mosque-attack-is-identified it wasn't  news 
returning reqs
visiting http://motherboard.vice.com/read/isis-vs-3d-printing
	priority is 0
	the current length of the visited set is : 5
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 15: ordinal not in range(128)

visiting http://munchies.vice.com/articles/why-is-brooklyn-barbecue-taking-over-the-world
	priority is 0
	the current length of the visited set is : 6
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 54: ordinal not in range(128)

visiting http://noisey.vice.com/blog/some-nerds-taught-a-computer-how-to-rap-deepbeat
	priority is 0
	the current length of the visited set is : 7
crawled http://noisey.vice.com/blog/some-nerds-taught-a-computer-how-to-rap-deepbeat it wasn't  news 
returning reqs
visiting http://store.vice.com/
	priority is 0
	the current length of the visited set is : 8
crawled http://store.vice.com/ it wasn't  news 
returning reqs
visiting http://www.vice.com/read/artists-of-today-part-2
	priority is 0
	the current length of the visited set is : 9
crawled http://www.vice.com/read/artists-of-today-part-2 it wasn't  news 
returning reqs
visiting https://news.vice.com/article/microbeads-kill-animals-and-destroy-the-environment-so-california-may-ban-them
	priority is 0
	the current length of the visited set is : 10
crawled https://news.vice.com/article/microbeads-kill-animals-and-destroy-the-environment-so-california-may-ban-them it was  news 
returning reqs
visiting http://motherboard.vice.com/terms
	priority is 0
	the current length of the visited set is : 11
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 7: ordinal not in range(128)

visiting https://news.vice.com/article/video-shows-frenzied-clash-at-protest-over-plan-to-build-massive-copper-mine-in-peru
	priority is 0
	the current length of the visited set is : 12
crawled https://news.vice.com/article/video-shows-frenzied-clash-at-protest-over-plan-to-build-massive-copper-mine-in-peru it wasn't  news 
returning reqs
visiting http://www.vice.com/author/batya-ungarsargon
	priority is 0
	the current length of the visited set is : 13
crawled http://www.vice.com/author/batya-ungarsargon it wasn't  news 
returning reqs
visiting http://www.vice.com/comics
	priority is 0
	the current length of the visited set is : 14
crawled http://www.vice.com/comics it wasn't  news 
returning reqs
visiting http://munchies.vice.com/articles/avocados-might-help-protect-your-lungs-from-stinky-air-pollution
	priority is 0
	the current length of the visited set is : 15
crawled http://munchies.vice.com/articles/avocados-might-help-protect-your-lungs-from-stinky-air-pollution it wasn't  news 
returning reqs
visiting https://twitter.com/vice
	priority is 0
	the current length of the visited set is : 16
crawled https://twitter.com/vice it wasn't  news 
returning reqs
visiting http://motherboard.vice.com/privacy
	priority is 0
	the current length of the visited set is : 17
crawled http://motherboard.vice.com/privacy it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/tipping-point
	priority is 2
	the current length of the visited set is : 18
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 43: ordinal not in range(128)

visiting http://www.vice.com/read/a-tech-guy-from-detroit-created-a-dating-app-that-matches-israelis-with-palestinians
	priority is 0
	the current length of the visited set is : 19
crawled http://www.vice.com/read/a-tech-guy-from-detroit-created-a-dating-app-that-matches-israelis-with-palestinians it wasn't  news 
returning reqs
visiting http://www.vice.com/read/the-us-military-euthanized-or-abandoned-thousands-of-their-own-canine-soldiers-at-the-end-of-the-vietnam-war-253
	priority is 0
	the current length of the visited set is : 20
crawled http://www.vice.com/read/the-us-military-euthanized-or-abandoned-thousands-of-their-own-canine-soldiers-at-the-end-of-the-vietnam-war-253 it was  news 
returning reqs
visiting https://news.vice.com/article/protesters-are-going-to-hit-the-streets-this-weekend-in-over-400-cities-to-march-against-monsanto
	priority is 0
	the current length of the visited set is : 21
crawled https://news.vice.com/article/protesters-are-going-to-hit-the-streets-this-weekend-in-over-400-cities-to-march-against-monsanto it was  news 
returning reqs
visiting http://munchies.vice.com/tag/bogota
	priority is 0
	the current length of the visited set is : 22
crawled http://munchies.vice.com/tag/bogota it wasn't  news 
returning reqs
visiting http://www.vice.com/author/aleksandar-hemon
	priority is 0
	the current length of the visited set is : 23
crawled http://www.vice.com/author/aleksandar-hemon it wasn't  news 
returning reqs
visiting https://www.facebook.com/VICE
	priority is 0
	the current length of the visited set is : 24
crawled https://www.facebook.com/VICE it wasn't  news 
returning reqs
visiting http://motherboard.vice.com/read/isis-wannabes-know-that-hacking-media-is-the-best-way-to-get-attention
	priority is 0
	the current length of the visited set is : 25
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting http://motherboard.vice.com/read/isis-wannabe-hacks-huffington-post-blogger
	priority is 0
	the current length of the visited set is : 26
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)

visiting https://news.vice.com/topic/plastic
	priority is 2
	the current length of the visited set is : 27
crawled https://news.vice.com/topic/plastic it wasn't  news 
returning reqs
visiting https://www.youtube.com/user/vice
	priority is 0
	the current length of the visited set is : 28
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 17: ordinal not in range(128)

visiting https://twitter.com/5gyres/statuses/598708102972715008
	priority is 2
	the current length of the visited set is : 29
crawled https://twitter.com/5gyres/statuses/598708102972715008 it wasn't  news 
returning reqs
visiting http://www.vice.com/read/the-inception-of-zombie-wars-0000654-v22n5
	priority is 0
	the current length of the visited set is : 30
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)

visiting http://munchies.vice.com/tag/barcelona
	priority is 0
	the current length of the visited set is : 31
crawled http://munchies.vice.com/tag/barcelona it wasn't  news 
returning reqs
visiting http://motherboard.vice.com/read/the-us-government-asked-sony-to-help-counter-isis-propaganda
	priority is 0
	the current length of the visited set is : 32
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 2: ordinal not in range(128)

visiting http://www.vice.com/stuff
	priority is 0
	the current length of the visited set is : 33
crawled http://www.vice.com/stuff it wasn't  news 
returning reqs
visiting http://munchies.vice.com/tag/beef
	priority is 0
	the current length of the visited set is : 34
crawled http://munchies.vice.com/tag/beef it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/pollution
	priority is 2
	the current length of the visited set is : 35
crawled https://news.vice.com/topic/pollution it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/great-plastic-garbage-patch
	priority is 2
	the current length of the visited set is : 36
crawled https://news.vice.com/topic/great-plastic-garbage-patch it wasn't  news 
returning reqs
visiting http://motherboard.vice.com/read/the-isis-cyberwar-hype-machine-is-doing-more-harm-than-good
	priority is 0
	the current length of the visited set is : 37
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 2: ordinal not in range(128)

visiting https://news.vice.com/topic/greenpeace
	priority is 2
	the current length of the visited set is : 38
crawled https://news.vice.com/topic/greenpeace it wasn't  news 
returning reqs
visiting http://munchies.vice.com/tag/bbq
	priority is 0
	the current length of the visited set is : 39
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting https://plus.google.com/+VICE
	priority is 0
	the current length of the visited set is : 41
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 8: ordinal not in range(128)

visiting http://www.vice.com/author/charlie-ambler
	priority is 0
	the current length of the visited set is : 42
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 4: ordinal not in range(128)

visiting http://www.vice.com/read/vice-exclusive-check-out-unsung-80s-post-punk-heroes-the-mothmen-with-their-new-reissue-815
	priority is 0
	the current length of the visited set is : 43
crawled http://www.vice.com/read/vice-exclusive-check-out-unsung-80s-post-punk-heroes-the-mothmen-with-their-new-reissue-815 it wasn't  news 
returning reqs
visiting http://motherboard.vice.com/read/future-mummies-will-be-dried-and-vacuum-packed
	priority is 0
	the current length of the visited set is : 44
crawled http://motherboard.vice.com/read/future-mummies-will-be-dried-and-vacuum-packed it wasn't  news 
returning reqs
visiting http://www.vice.com/video/vice-meets-anas-aremeyaw-anas
	priority is 0
	the current length of the visited set is : 45
crawled http://www.vice.com/video/vice-meets-anas-aremeyaw-anas it wasn't  news 
returning reqs
visiting http://www.marketplace.org/topics/business/pollutants-your-face-wash
	priority is 2
	the current length of the visited set is : 46
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 9: ordinal not in range(128)

visiting https://twitter.com/HammerDaily
	priority is 2
	the current length of the visited set is : 47
crawled https://twitter.com/HammerDaily it wasn't  news 
returning reqs
visiting http://storyofstuff.org/movies/
	priority is 2
	the current length of the visited set is : 48
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://www.safetyandcarecommitment.com/ingredient-info/other/microbeads
	priority is 2
	the current length of the visited set is : 49
crawled http://www.safetyandcarecommitment.com/ingredient-info/other/microbeads it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/microbeads
	priority is 2
	the current length of the visited set is : 50
crawled https://news.vice.com/topic/microbeads it wasn't  news 
returning reqs
visiting http://motherboard.vice.com/read/about-motherboard
	priority is 0
	the current length of the visited set is : 51
crawled http://motherboard.vice.com/read/about-motherboard it wasn't  news 
returning reqs
visiting http://education.nationalgeographic.com/education/encyclopedia/great-pacific-garbage-patch/?ar_a=1
	priority is 0
	the current length of the visited set is : 52
crawled http://education.nationalgeographic.com/education/encyclopedia/great-pacific-garbage-patch/?ar_a=1 it wasn't  news 
returning reqs
visiting https://news.vice.com/article/plastic-microbeads-from-body-wash-are-contaminating-the-great-lakes
	priority is 2
	the current length of the visited set is : 53
crawled https://news.vice.com/article/plastic-microbeads-from-body-wash-are-contaminating-the-great-lakes it was  news 
returning reqs
visiting https://news.vice.com/topic/oceans
	priority is 2
	the current length of the visited set is : 54
crawled https://news.vice.com/topic/oceans it wasn't  news 
returning reqs
visiting https://twitter.com/josiahmhesse
	priority is 2
	the current length of the visited set is : 55
crawled https://twitter.com/josiahmhesse it wasn't  news 
returning reqs
visiting http://www.vice.com/film
	priority is 0
	the current length of the visited set is : 56
crawled http://www.vice.com/film it wasn't  news 
returning reqs
visiting https://news.vice.com/article/%3C/p
	priority is 2
	the current length of the visited set is : 57
crawled https://news.vice.com/article/%3C/p it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/syngenta
	priority is 2
	the current length of the visited set is : 58
crawled https://news.vice.com/topic/syngenta it wasn't  news 
returning reqs
visiting http://www.rollingstone.com/music/news/neil-young-recording-new-album-with-willie-nelsons-sons-20150108
	priority is 2
	the current length of the visited set is : 59
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 95: ordinal not in range(128)

visiting http://www.vice.com/read/canadas-truth-commission-on-residential-schools-is-coming-to-a-troubling-close-far-from-reconciliation
	priority is 0
	the current length of the visited set is : 60
crawled http://www.vice.com/read/canadas-truth-commission-on-residential-schools-is-coming-to-a-troubling-close-far-from-reconciliation it was  news 
returning reqs
visiting http://www.vice.com/read/pc-andrew-ott-conviction-235
	priority is 0
	the current length of the visited set is : 61
crawled http://www.vice.com/read/pc-andrew-ott-conviction-235 it was  news 
returning reqs
visiting https://news.vice.com/topic/reverend-billy
	priority is 2
	the current length of the visited set is : 62
crawled https://news.vice.com/topic/reverend-billy it wasn't  news 
returning reqs
visiting http://www.vice.com/read/the-sand-looters-0000647-v22n5
	priority is 0
	the current length of the visited set is : 63
crawled http://www.vice.com/read/the-sand-looters-0000647-v22n5 it wasn't  news 
returning reqs
visiting http://www.cnn.com/2010/LIVING/02/12/war.dogs/
	priority is 2
	the current length of the visited set is : 64
crawled http://www.cnn.com/2010/LIVING/02/12/war.dogs/ it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140519-dogs-war-canines-soldiers-troops-military-vietnam/
	priority is 2
	the current length of the visited set is : 65
crawled http://news.nationalgeographic.com/news/2014/05/140519-dogs-war-canines-soldiers-troops-military-vietnam/ it was  news 
returning reqs
visiting https://news.vice.com/topic/monarch-butterflies
	priority is 2
	the current length of the visited set is : 66
crawled https://news.vice.com/topic/monarch-butterflies it wasn't  news 
returning reqs
visiting http://www.denverpost.com/news/ci_6003386
	priority is 2
	the current length of the visited set is : 67
crawled http://www.denverpost.com/news/ci_6003386 it wasn't  news 
returning reqs
visiting http://www.washingtonpost.com/news/to-your-health/wp/2014/09/18/why-dentists-are-speaking-out-about-the-plastic-beads-in-your-toothpaste/
	priority is 2
	the current length of the visited set is : 68
crawled http://www.washingtonpost.com/news/to-your-health/wp/2014/09/18/why-dentists-are-speaking-out-about-the-plastic-beads-in-your-toothpaste/ it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/california
	priority is 2
	the current length of the visited set is : 69
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 40: ordinal not in range(128)

visiting http://www.beatthemicrobead.org/en/science
	priority is 2
	the current length of the visited set is : 70
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 14: ordinal not in range(128)

visiting https://news.vice.com/topic/bees
	priority is 2
	the current length of the visited set is : 71
crawled https://news.vice.com/topic/bees it wasn't  news 
returning reqs
visiting http://asmdc.org/members/a50/news-room/press-releases/legislation-to-ban-plastic-microbeads-in-care-products-passes-committee
	priority is 2
	the current length of the visited set is : 72
crawled http://asmdc.org/members/a50/news-room/press-releases/legislation-to-ban-plastic-microbeads-in-care-products-passes-committee it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/agriculture
	priority is 2
	the current length of the visited set is : 73
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 39: ordinal not in range(128)

visiting https://twitter.com/NatGeo
	priority is 2
	the current length of the visited set is : 74
crawled https://twitter.com/NatGeo it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/herbicides
	priority is 2
	the current length of the visited set is : 75
crawled https://news.vice.com/topic/herbicides it wasn't  news 
returning reqs
visiting http://www.vice.com/pages/privacy-and-terms
	priority is 0
	the current length of the visited set is : 76
crawled http://www.vice.com/pages/privacy-and-terms it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/pesticides
	priority is 2
	the current length of the visited set is : 77
crawled https://news.vice.com/topic/pesticides it wasn't  news 
returning reqs
visiting http://www.vice.com/pages/about
	priority is 0
	the current length of the visited set is : 78
crawled http://www.vice.com/pages/about it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/neonicotinoids
	priority is 2
	the current length of the visited set is : 79
crawled https://news.vice.com/topic/neonicotinoids it wasn't  news 
returning reqs
visiting http://www.vice.com/pages/jobs
	priority is 0
	the current length of the visited set is : 80
crawled http://www.vice.com/pages/jobs it wasn't  news 
returning reqs
visiting https://www.facebook.com/natgeo
	priority is 2
	the current length of the visited set is : 81
crawled https://www.facebook.com/natgeo it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/monsanto
	priority is 2
	the current length of the visited set is : 82
crawled https://news.vice.com/topic/monsanto it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/05/150513-north-sea-ngbooktalk-vikings-bergen-herring-money-roman-law-dark-ages-donald-trump/
	priority is 0
	the current length of the visited set is : 83
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 16: ordinal not in range(128)

visiting http://www.vice.com/tag/lgbt
	priority is 0
	the current length of the visited set is : 84
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting http://www.vice.com/rss
	priority is 0
	the current length of the visited set is : 85
crawled http://www.vice.com/rss it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/culture
	priority is 0
	the current length of the visited set is : 86
crawled http://www.vice.com/tag/culture it wasn't  news 
returning reqs
visiting https://twitter.com/WailQ
	priority is 2
	the current length of the visited set is : 87
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting http://www.vice.com/tag/opinion
	priority is 0
	the current length of the visited set is : 88
crawled http://www.vice.com/tag/opinion it wasn't  news 
returning reqs
visiting https://news.vice.com/article/nearly-300000-tons-of-plastic-is-floating-in-the-worlds-oceans
	priority is 2
	the current length of the visited set is : 89
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 89: ordinal not in range(128)

visiting https://news.vice.com/article/bees-might-be-addicted-to-nicotine-like-insecticides-that-are-killing-them
	priority is 2
	the current length of the visited set is : 90
crawled https://news.vice.com/article/bees-might-be-addicted-to-nicotine-like-insecticides-that-are-killing-them it was  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140518-dogs-war-canines-soldiers-troops-military-japanese-prisoner/
	priority is 2
	the current length of the visited set is : 91
crawled http://news.nationalgeographic.com/news/2014/05/140518-dogs-war-canines-soldiers-troops-military-japanese-prisoner/ it was  news 
returning reqs
visiting https://news.vice.com/article/the-most-widely-used-herbicide-in-the-united-states-could-cause-cancer-in-humans-says-a-world-health-organization-study
	priority is 2
	the current length of the visited set is : 92
crawled https://news.vice.com/article/the-most-widely-used-herbicide-in-the-united-states-could-cause-cancer-in-humans-says-a-world-health-organization-study it was  news 
returning reqs
visiting http://www.vice.com/tag/politics
	priority is 0
	the current length of the visited set is : 93
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 13: ordinal not in range(128)

visiting http://www.amazon.com/War-Dogs-Canine-Heroism-History/dp/1137279680
	priority is 0
	the current length of the visited set is : 94
crawled http://www.amazon.com/War-Dogs-Canine-Heroism-History/dp/1137279680 it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/food-chain
	priority is 2
	the current length of the visited set is : 95
crawled https://news.vice.com/topic/food-chain it wasn't  news 
returning reqs
visiting http://www.vice.com/series/the-vice-guide-to-mental-health
	priority is 0
	the current length of the visited set is : 96
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6: ordinal not in range(128)

visiting https://news.vice.com/topic/great-lakes
	priority is 2
	the current length of the visited set is : 97
crawled https://news.vice.com/topic/great-lakes it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/05/150516-syria-palmyra-islamic-state-ISIS-ISIL-ancient-Rome-destruction-world-heritage-archaeology/
	priority is 0
	the current length of the visited set is : 98
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 14: ordinal not in range(128)

visiting https://news.vice.com/contributor/theodore-hamm
	priority is 2
	the current length of the visited set is : 99
crawled https://news.vice.com/contributor/theodore-hamm it wasn't  news 
returning reqs
visiting http://www.vice.com/tag/crime
	priority is 0
	the current length of the visited set is : 100
crawled http://www.vice.com/tag/crime it wasn't  news 
returning reqs
visiting http://www.vice.com/magazine
	priority is 0
	the current length of the visited set is : 101
crawled http://www.vice.com/magazine it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/facial-scrubs
	priority is 2
	the current length of the visited set is : 102
crawled https://news.vice.com/topic/facial-scrubs it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140517-dogs-war-canines-soldiers-troops-marine-military-pacific-japan/
	priority is 2
	the current length of the visited set is : 103
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 12: ordinal not in range(128)

visiting https://instagram.com/natgeo/
	priority is 0
	the current length of the visited set is : 104
crawled https://instagram.com/natgeo/ it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140516-dogs-war-canines-soldiers-troops-army-military/
	priority is 2
	the current length of the visited set is : 105
crawled http://news.nationalgeographic.com/news/2014/05/140516-dogs-war-canines-soldiers-troops-army-military/ it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/hand-soap
	priority is 2
	the current length of the visited set is : 106
crawled https://news.vice.com/topic/hand-soap it wasn't  news 
returning reqs
visiting http://www.vice.com/photos
	priority is 0
	the current length of the visited set is : 107
crawled http://www.vice.com/photos it wasn't  news 
returning reqs
visiting https://twitter.com/ldattaro
	priority is 2
	the current length of the visited set is : 108
crawled https://twitter.com/ldattaro it wasn't  news 
returning reqs
visiting http://www.defendtherighttoprotest.org/police-on-trial-for-2010-student-fees-protests-ipcc-release-audio-recording/
	priority is 2
	the current length of the visited set is : 109
crawled http://www.defendtherighttoprotest.org/police-on-trial-for-2010-student-fees-protests-ipcc-release-audio-recording/ it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/05/150514-indiana-jones-archaeology-exhibit-national-geographic-museum/
	priority is 0
	the current length of the visited set is : 110
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 9: ordinal not in range(128)

visiting https://news.vice.com/topic/body-wash
	priority is 2
	the current length of the visited set is : 111
crawled https://news.vice.com/topic/body-wash it wasn't  news 
returning reqs
visiting http://www.vice.com/dnd
	priority is 0
	the current length of the visited set is : 112
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 243: ordinal not in range(128)

visiting https://news.vice.com/topic/united-states
	priority is 2
	the current length of the visited set is : 113
crawled https://news.vice.com/topic/united-states it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/
	priority is 2
	the current length of the visited set is : 114
crawled http://news.nationalgeographic.com/ it wasn't  news 
returning reqs
visiting http://www.vice.com/nsfw
	priority is 0
	the current length of the visited set is : 115
crawled http://www.vice.com/nsfw it wasn't  news 
returning reqs
visiting https://twitter.com/aaronmiguel_
	priority is 2
	the current length of the visited set is : 116
crawled https://twitter.com/aaronmiguel_ it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/fungicides
	priority is 2
	the current length of the visited set is : 117
crawled https://news.vice.com/topic/fungicides it wasn't  news 
returning reqs
visiting http://www.vice.com/gaming
	priority is 0
	the current length of the visited set is : 118
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting https://news.vice.com/topic/bayer
	priority is 2
	the current length of the visited set is : 119
crawled https://news.vice.com/topic/bayer it wasn't  news 
returning reqs
visiting http://www.thelancet.com/journals/lanonc/article/PIIS1470-2045(15)
	priority is 2
	the current length of the visited set is : 120
crawled http://www.thelancet.com/journals/lanonc/article/PIIS1470-2045(15) it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/04/150415-ngbooktalk-nazis-auschwitz-holocaust-survivors/
	priority is 0
	the current length of the visited set is : 121
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting https://news.vice.com/topic/colony-collapse
	priority is 2
	the current length of the visited set is : 122
crawled https://news.vice.com/topic/colony-collapse it wasn't  news 
returning reqs
visiting http://www.vice.com/food
	priority is 0
	the current length of the visited set is : 123
crawled http://www.vice.com/food it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/insecticides
	priority is 2
	the current length of the visited set is : 124
crawled https://news.vice.com/topic/insecticides it wasn't  news 
returning reqs
visiting http://www.vice.com/tech
	priority is 0
	the current length of the visited set is : 125
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 3: ordinal not in range(128)

visiting http://www.pdsa.org.uk/what-we-do/animal-honours/pdsa-dickin-medal
	priority is 0
	the current length of the visited set is : 126
crawled http://www.pdsa.org.uk/what-we-do/animal-honours/pdsa-dickin-medal it wasn't  news 
returning reqs
visiting http://www.vice.com/sports
	priority is 0
	the current length of the visited set is : 127
crawled http://www.vice.com/sports it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/bumblebees
	priority is 2
	the current length of the visited set is : 128
crawled https://news.vice.com/topic/bumblebees it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/2015/04/150418-abraham-lincoln-funeral-train-railroad-civil-war-history/
	priority is 0
	the current length of the visited set is : 129
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 311, in _pair_iter
    for el in it:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 7: ordinal not in range(128)

visiting http://www.nature.com/nature/journal/v521/n7550/full/nature14414.html
	priority is 0
	the current length of the visited set is : 130
crawled http://www.nature.com/nature/journal/v521/n7550/full/nature14414.html it was  news 
returning reqs
visiting http://www.vice.com/travel
	priority is 0
	the current length of the visited set is : 131
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)

visiting http://news.nationalgeographic.com/2015/05/150508-wwii-planes-flyover-veterans-ve-day-victory-in-europe-video/
	priority is 0
	the current length of the visited set is : 132
crawled http://news.nationalgeographic.com/2015/05/150508-wwii-planes-flyover-veterans-ve-day-victory-in-europe-video/ it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/honeybees
	priority is 2
	the current length of the visited set is : 133
crawled https://news.vice.com/topic/honeybees it wasn't  news 
returning reqs
visiting https://twitter.com/mcslo
	priority is 2
	the current length of the visited set is : 134
crawled https://twitter.com/mcslo it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/epa
	priority is 2
	the current length of the visited set is : 135
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 67: ordinal not in range(128)

visiting http://www.vice.com/fashion
	priority is 0
	the current length of the visited set is : 136
crawled http://www.vice.com/fashion it wasn't  news 
returning reqs
visiting http://www.nature.com/nature/journal/v521/n7550/full/nature14420.html
	priority is 0
	the current length of the visited set is : 137
crawled http://www.nature.com/nature/journal/v521/n7550/full/nature14420.html it was  news 
returning reqs
visiting https://news.vice.com/topic/cancer
	priority is 2
	the current length of the visited set is : 138
crawled https://news.vice.com/topic/cancer it wasn't  news 
returning reqs
visiting http://www.vice.com/music
	priority is 0
	the current length of the visited set is : 139
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)

visiting https://www.flickr.com/photos/kumaravel/8475706265/in/photolist-geqSxG-anA9LU-aZQswr-rydUL6-keqNav-51dnrz-9pJEE6-nqm6pq-9gkuut-6G74xK-n3PAzS-8Ws8PR-psDtRA-psrCVf-dUYdbD-5ye11b-b5XiGD-oCpSV3-qtQ6At-9AsaH8-gyXhs-bEPEnU-nwpcWw-36NnU-6FD55A-2rk12-g3RcZX-c1o3cW-j5zBVt-4EKHC-cWvdRG-pCQd4t-njACwd-fd5WmT-nkkFcB-2bQ5mq-2RhYgn-agiVk7-bRYUcB-dasASJ-hfFPDJ-qyC7dT-hRoQi-84mbyM-amqves-oFmvN7-nhQLfX-o5tvBD-gDeygw-pnGhRV
	priority is 2
	the current length of the visited set is : 140
crawled https://www.flickr.com/photos/kumaravel/8475706265/in/photolist-geqSxG-anA9LU-aZQswr-rydUL6-keqNav-51dnrz-9pJEE6-nqm6pq-9gkuut-6G74xK-n3PAzS-8Ws8PR-psDtRA-psrCVf-dUYdbD-5ye11b-b5XiGD-oCpSV3-qtQ6At-9AsaH8-gyXhs-bEPEnU-nwpcWw-36NnU-6FD55A-2rk12-g3RcZX-c1o3cW-j5zBVt-4EKHC-cWvdRG-pCQd4t-njACwd-fd5WmT-nkkFcB-2bQ5mq-2RhYgn-agiVk7-bRYUcB-dasASJ-hfFPDJ-qyC7dT-hRoQi-84mbyM-amqves-oFmvN7-nhQLfX-o5tvBD-gDeygw-pnGhRV it wasn't  news 
returning reqs
visiting http://www.vice.com/news
	priority is 0
	the current length of the visited set is : 141
crawled http://www.vice.com/news it wasn't  news 
returning reqs
visiting http://www.vice.com/videos
	priority is 0
	the current length of the visited set is : 142
crawled http://www.vice.com/videos it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/roundup
	priority is 2
	the current length of the visited set is : 143
crawled https://news.vice.com/topic/roundup it wasn't  news 
returning reqs
visiting https://news.vice.com/topic/glyphosate
	priority is 2
	the current length of the visited set is : 144
crawled https://news.vice.com/topic/glyphosate it wasn't  news 
returning reqs
visiting http://www.canada.com/victoriatimescolonist/news/comment/story.html
	priority is 2
	the current length of the visited set is : 145
crawled http://www.canada.com/victoriatimescolonist/news/comment/story.html it wasn't  news 
returning reqs
visiting http://www.cbc.ca/news/canada/manitoba/canada-s-truth-commission-learned-from-mandela-says-head-1.2454851
	priority is 2
	the current length of the visited set is : 146
crawled http://www.cbc.ca/news/canada/manitoba/canada-s-truth-commission-learned-from-mandela-says-head-1.2454851 it wasn't  news 
returning reqs
visiting http://www.thestar.com/news/canada/2012/06/11/four_years_later_harpers_apology_for_residential_schools_rings_hollow_for_many.html
	priority is 2
	the current length of the visited set is : 147
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 10: ordinal not in range(128)

visiting http://www.cbc.ca/news/canada/prime-minister-stephen-harper-s-statement-of-apology-1.734250
	priority is 2
	the current length of the visited set is : 148
crawled http://www.cbc.ca/news/canada/prime-minister-stephen-harper-s-statement-of-apology-1.734250 it wasn't  news 
returning reqs
visiting http://news.nationalgeographic.com/news/2014/05/140520-myanmar-kachin-independence-army-illegal-logging-teak-china-world/
	priority is 0
	the current length of the visited set is : 149
crawled http://news.nationalgeographic.com/news/2014/05/140520-myanmar-kachin-independence-army-illegal-logging-teak-china-world/ it was  news 
returning reqs
visiting http://www.theglobeandmail.com/news/national/in-photos-st-michaels-residential-school-demolition-ceremony/article23067843/
	priority is 2
	the current length of the visited set is : 150
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 155, in goahead
    if i < j: self.handle_data(rawdata[i:j])
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 47, in handle_data
    if len(blob.sentences) > 1 or blob.sentences[0][-1] in PageParser.CLOSING_PUNC:
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 24, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 601, in sentences
    return self._create_sentence_objects()
  File "C:\Python27\lib\site-packages\textblob\blob.py", line 645, in _create_sentence_objects
    sentences = sent_tokenize(self.raw)
  File "C:\Python27\lib\site-packages\textblob\base.py", line 64, in itokenize
    return (t for t in self.tokenize(text, *args, **kwargs))
  File "C:\Python27\lib\site-packages\textblob\decorators.py", line 35, in decorated
    return func(*args, **kwargs)
  File "C:\Python27\lib\site-packages\textblob\tokenizers.py", line 57, in tokenize
    return nltk.tokenize.sent_tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\__init__.py", line 86, in sent_tokenize
    return tokenizer.tokenize(text)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1226, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1274, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1265, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1304, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1280, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1325, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 1460, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 310, in _pair_iter
    prev = next(it)
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 577, in _annotate_first_pass
    for aug_tok in tokens:
  File "C:\Python27\lib\site-packages\nltk\tokenize\punkt.py", line 542, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 11: ordinal not in range(128)

visiting http://reconciliationcanada.ca/welcome/who-we-are/
	priority is 2
	the current length of the visited set is : 151
	EXCEPTION!!!! 

		 traceback:Traceback (most recent call last):
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\newsspider.py", line 109, in process_node
    doc = frame_features(response.body,features=features, dg=self.dg)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\__init__.py", line 88, in frame_features
    parsed_text, html_tag_counts = PageParser.parse(text)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 57, in parse
    return p.process(html)
  File "C:\Users\William\Desktop\CS491Proj\491_Spider\crawltest\spiders\mltools\pageparser.py", line 51, in process
    self.feed(html)
  File "C:\Python27\lib\HTMLParser.py", line 117, in feed
    self.goahead(0)
  File "C:\Python27\lib\HTMLParser.py", line 161, in goahead
    k = self.parse_starttag(i)
  File "C:\Python27\lib\HTMLParser.py", line 308, in parse_starttag
    attrvalue = self.unescape(attrvalue)
  File "C:\Python27\lib\HTMLParser.py", line 475, in unescape
    return re.sub(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));", replaceEntities, s)
  File "C:\Python27\lib\re.py", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 323: ordinal not in range(128)

visiting http://www.cbc.ca/news/canada/manitoba/truth-is-hard-but-residential-school-reconciliation-harder-murray-sinclair-1.2819931
	priority is 2
	the current length of the visited set is : 152
crawled http://www.cbc.ca/news/canada/manitoba/truth-is-hard-but-residential-school-reconciliation-harder-murray-sinclair-1.2819931 it wasn't  news 
returning reqs
visiting http://www.canada.com/ottawacitizen/story.html
	priority is 2
	the current length of the visited set is : 153
crawled http://www.canada.com/ottawacitizen/story.html it wasn't  news 
returning reqs
visiting http://www.scielo.org.za/scielo.php
	priority is 2
	the current length of the visited set is : 154
